[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Elementary Statistics with R Programming",
    "section": "",
    "text": "Preface\nThis is an R-manual that accompanies the textbook Triola (2022) for the courses STAT 2670: Elementary Statistics offered at Auburn University at Montgomery.\nCredits:\n\nJerome Goddard (Chapters 2 and 3)\nYi Wang (Chapters 1, 4 and 5; Overall editing)\nWen Tang (Chapters 6 and 7)\nJieun Park (Chapters 8 and 10)\n\n\n\n\n\nTriola, Mario F. 2022. Elementary Statistics. USA: Pearson.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html#setting-up-your-own-computing-environment-on-a-personal-computer",
    "href": "intro.html#setting-up-your-own-computing-environment-on-a-personal-computer",
    "title": "Setting-up Computing Environment",
    "section": "Setting up your own computing environment on a personal computer",
    "text": "Setting up your own computing environment on a personal computer\nThis is the recommended way and the advantage is that it’s easy to handle files.\n\nGo to the website https://posit.co/download/rstudio-desktop/.\nFollow the two steps: 1) download and install R: Choose the appropriate operating system, and then choose “base” to “install R for the first time”. You can simply accept all default options.\n\n\ndownload Rstudio Desktop and Install it.\n\nAfter installation, start R-Studio, and you are ready to use it.",
    "crumbs": [
      "Setting-up Computing Environment"
    ]
  },
  {
    "objectID": "intro.html#use-r-studio-cloud-no-setting-up-needed",
    "href": "intro.html#use-r-studio-cloud-no-setting-up-needed",
    "title": "Setting-up Computing Environment",
    "section": "Use R-Studio Cloud (No setting-up needed)",
    "text": "Use R-Studio Cloud (No setting-up needed)\nAlternatively, one can save the hassle of setting up on a personal computer and use the R-Studio Cloud for free. Here are the steps:\n\nGo to the website https://login.rstudio.cloud/.\nEither create a new account using an email address such as your AUM email or simply “Log in using Google” or click on other log-in alternative.\n\nAfter log-in to your account, you are ready to use R Studio.",
    "crumbs": [
      "Setting-up Computing Environment"
    ]
  },
  {
    "objectID": "quick-r.html#minture-r-programming-introduction",
    "href": "quick-r.html#minture-r-programming-introduction",
    "title": "1  Quick Start to R Programming",
    "section": "1.1 1-minture R Programming Introduction",
    "text": "1.1 1-minture R Programming Introduction\nVariables: To create a variable, use the assignment operator &lt;-. For example,\n\n\nCode\nx &lt;- 5\n\n\nData Structures: R supports various data structures such as vectors, matrices, arrays, lists, and dataframes. For instance, you can create a vector using c() like\n\n\nCode\nmy_vector &lt;- c(1, 2, 3, 4, 5)\n\n\nTo index the second element of my_vector, use [] operator:\n\n\nCode\nmy_vector[2]\n\n\n[1] 2\n\n\nThe output after running the code my_vector[2] is displayed as [1] 2, where [1] indicates the first line of the output, and 2 is the value of my_vector[2].\nTo print a formatted output, use the built-in function cat to concatenate strings enclosed in double quotes \"\"(or equivalently single quotes ' '). Note \"\\n\" represents a newline feed. For example:\n\n\nCode\ncat(\"The first element in my_vector is:\", my_vector[1], \"\\n\")\n\n\nThe first element in my_vector is: 1 \n\n\nFunctions: Functions in R are defined using the function() keyword. For example, you can create a function as follows:\n\n\nCode\nmy_sum &lt;- function(arg1, arg2) {\n  # function body\n  return(arg1 + arg2)  #return the sum of arg1 and arg2\n}\n\n\nCode comment: A code comment starts with #. A comment line will not affect your code. When a R-code is executed, a comment line will be ignored by the R-code interpreter. When you are following along with the code in this manual, you do not need to type the line starting with #. They are provided to interpret the codes.\nControl Structures: R supports typical control structures like if-else statements, for loops, while loops, etc.\nPackages: R’s functionality can be extended through packages. You can install packages using the install.packages(\"package_Name\") function and load them using the library(\"package_name\") function.\nData Manipulation and Analysis: R provides powerful tools for data manipulation and analysis. Packages like dplyr and ggplot2 are commonly used for data manipulation and visualization, respectively.\nHelp: to access the help document, type in the R-console: ?function_name or help(function_name). For example:, ?mean or help(mean).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quick Start to R Programming</span>"
    ]
  },
  {
    "objectID": "ch2.html#frequency-distribution-shows-the-count-or-frequency-of-each-unique-value-or-category-in-a-dataset-providing-a-clear-picture-of-how-data-is-distributed-across-different-values-or-groups.",
    "href": "ch2.html#frequency-distribution-shows-the-count-or-frequency-of-each-unique-value-or-category-in-a-dataset-providing-a-clear-picture-of-how-data-is-distributed-across-different-values-or-groups.",
    "title": "2  Exploring Data with Tables and Graphs",
    "section": "2.1 Frequency distribution shows the count or frequency of each unique value or category in a dataset, providing a clear picture of how data is distributed across different values or groups.",
    "text": "2.1 Frequency distribution shows the count or frequency of each unique value or category in a dataset, providing a clear picture of how data is distributed across different values or groups.\n\n2.1.1 Frequency distributions\nThe R command table() will generate a frequency distribution for any data set. Let’s analyze example test scores from a fictional math class. Notice the first row of the output is the data name, the second row is the actual data, and the third row contains the number of times each data value appears.\n\n\nCode\n# Load test data into a variable names scores\nscores = c(95, 90, 85, 85, 87, 74, 75, 64, 85, 84, 87, 15, 20, 75, 75, 90, 75)\n\n# Create a frequency table for the scores data\ntable(scores)\n\n\nscores\n15 20 64 74 75 84 85 87 90 95 \n 1  1  1  1  4  1  3  2  2  1 \n\n\n\n\n2.1.2 Relative frequency distributions\nRelative frequency distributions give similar information as a frequency distribution except they use percentages. Let’s examine the same scores data set defined above. Notice in the output that the second row is the actual data and the third row contains the relative frequencies (rounded to two decimal places).\n\n\nCode\n# Create a relative frequency table for the scores data\nrftable &lt;- table(scores)/length(scores)\nround(rftable, digits = 2)\n\n\nscores\n  15   20   64   74   75   84   85   87   90   95 \n0.06 0.06 0.06 0.06 0.24 0.06 0.18 0.12 0.12 0.06",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "ch2.html#histograms",
    "href": "ch2.html#histograms",
    "title": "2  Exploring Data with Tables and Graphs",
    "section": "2.2 Histograms",
    "text": "2.2 Histograms\nA histogram is a bar chart that shows how often different values occur in a dataset.\n\n2.2.1 Histogram\nThe command hist() will generate a histogram for any data. Here is an example using our scores data from above. Notice the x-axis represents the actual scores and the y-axis shows the frequency of the data points. We will use the following command options: 1) main allows the title to be specified, 2) xlab sets the x-axis label, and 3) ylab sets the y-axis label.\n\n\nCode\n# Create a histogram and customize the axis labels and title\n# main is the Plot title, xlab is the x-axis label, & ylab is the y-axis label\nhist(scores, main = \"Histogram for test scores\", xlab = \"Test Scores\", \n     ylab = \"Frequency\")\n\n\n\n\n\n\n\n2.2.2 Relative frequency histogram\nA relative histogram is a bar chart that displays the proportion or percentage of values in different bins within a dataset, providing a relative view of the data distribution.\n\n\nCode\n# Using freq = FALSE in hist() will create a relative frequency histogram\nhist(scores, freq = FALSE, main = \"Relative frequency histogram\", \n     xlab = \"Test Scores\", ylab = \"Relative Frequency\")\n\n\n\n\n\n\n\n2.2.3 Common distributions\nNormal distributions are bell-shaped and symmetrical, uniform distributions have constant probabilities across a range, skewed right distributions are characterized by a long tail on the right side, and skewed left distributions have a long tail on the left side, each exhibiting distinct patterns of data distribution. We will use the hist() command to explore each of these common distributions in the code below.\n\n\nCode\n# Sample normal distribution\nn &lt;- 100\nmean &lt;- 69\nsd &lt;- 3.6\nnormalData &lt;- rnorm(n, mean, sd)\n \n# Sample uniform distribution using the command runif\nuniformData &lt;- runif(50000, min = 10, max = 11)\n\n# Sample of a distribution that is skewed right\nskewedRightData &lt;- rexp(1000, 0.4)\n\n# Sample of a distribution that is skewed left\nskewedLeftData &lt;- 1 - rexp(1000, 0.2)\n\n# Create histogram of normal data\nhist(normalData, main = \"Normal distribution\")\n\n\n\n\n\nCode\n# Create histogram of uniform data\nhist(uniformData, main = \"Uniform distribution\")\n\n\n\n\n\nCode\n# Create histogram of skewed right data\nhist(skewedRightData, main = \"Distribution that is skewed right\")\n\n\n\n\n\nCode\n# Create histogram of skewed left data\nhist(skewedLeftData, main = \"Distribution that is skewed left\")\n\n\n\n\n\n\n\n2.2.4 Normal quantile plots\nA normal quantile plot, also known as a Q-Q plot, is a graphical tool used to assess whether a dataset follows a normal distribution by comparing its quantiles (ordered values) to the quantiles of a theoretical normal distribution; if the points closely follow a straight line, the data is approximately normal. Let’s use the commands qqnorm() and qqline() to visually test which data set is most likely a sample from a normal distribution.\n\n\nCode\n# Test normalData from above\nqqnorm(normalData, main = \"Q-Q Plot for normalData\")\nqqline(normalData)\n\n\n\n\n\nNotice that the normalData Q-Q plot shows the points close to the Q-Q line over the entire x-axis.\n\n\nCode\n# Test uniformData from above\nqqnorm(uniformData, main = \"Q-Q Plot for uniformData\")\nqqline(uniformData)\n\n\n\n\n\nFor the uniformData dataset, the Q-Q plot shows good agreement between points and line in the center (around 0) but not on either left or right of the x-axis.\n\n\n2.2.5 Let’s put it all together!\nIn the built-in R dataset ChickWeight, weights are taken from several groups of chickens that were fed various diets. We are asked to use both histogram and Q-Q plots to determine if weights from group 1 and 4 are approximately normal, uniform, skewed left, or skewed right.\n\n\nCode\n# Load data from the built-in dataset into a variable named ChickWeight\ndata(\"ChickWeight\")\n\n# Extract all weights from group 1\ngroup1Weights &lt;- ChickWeight[ChickWeight$Diet == 1, 1]\n\n# Extract all weights from group 4\ngroup4Weights &lt;- ChickWeight[ChickWeight$Diet == 4, 1]\n\n# Create a histogram of weights from group 1\nhist(group1Weights, main = \"Group 1 weights\", xlab = \"Weight\", ylab = \"Frequency\")\n\n\n\n\n\nCode\n# Create a histogram of weights from group 4\nhist(group4Weights, main = \"Group 4 weights\", xlab = \"Weight\", ylab = \"Frequency\")\n\n\n\n\n\nIs the group 1 distribution approximately normal or would a different distribution be a better fit? What about group 4? Now, let’s confirm our results using Q-Q plots.\n\n\nCode\n# Test group1Weights from above\nqqnorm(group1Weights, main = \"Q-Q Plot for Group 1\")\nqqline(group1Weights)\n\n\n\n\n\nCode\n# Test group4Weights from above\nqqnorm(group4Weights, main = \"Q-Q Plot for Group 4\")\nqqline(group4Weights)\n\n\n\n\n\nDoes the Q-Q plot confirm your guess from our visual inspection? Which group is closer to a normal distribution?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "ch2.html#graphs-that-enlighten-and-graphs-that-deceive",
    "href": "ch2.html#graphs-that-enlighten-and-graphs-that-deceive",
    "title": "2  Exploring Data with Tables and Graphs",
    "section": "2.3 Graphs that enlighten and graphs that deceive",
    "text": "2.3 Graphs that enlighten and graphs that deceive\nR has many commands to illustrate data revealing hidden patterns that could be otherwise missed. We will explore several of these commands using three different datasets:\n\nChicken Weights: Same data used in Section 2.2: two different groups of chickens fed with different feed.\nAirline Passengers: A time series of the number of airline passengers in the US by month.\nUS Personal Expenditure Average personal expenditures for adults in the US from 1960.\n\nBelow we will load these data sets when we need them.\n\n2.3.1 Dotplot\nA dotplot is a simple graphical representation of data in which each data point is shown as a dot above its corresponding value on a number line, helping to visualize the distribution and identify patterns in a dataset. With our data previously loaded from the previous run, let’s create a dotplot of the data. First for weights of both groups of chickens.\n\n\nCode\n# Chicken weights:\n# Load data from the built-in dataset into a variable named ChickWeight\ndata(\"ChickWeight\")\n\n# Extract all weights from group 1\ngroup1Weights &lt;- ChickWeight[ChickWeight$Diet == 1, 1]\n\n# Extract all weights from group 4\ngroup4Weights &lt;- ChickWeight[ChickWeight$Diet == 4, 1]\n\n# Dotplot for group 1 chickens\ndotchart(group1Weights, main = \"Dotplot of Group 1 chicken weights\", xlab = \"Weight\")\n\n\n\n\n\nCode\n# Dotplot for group 4 chickens\ndotchart(group4Weights, main = \"Dotplot of Group 4 chicken weights\", xlab = \"Weight\")\n\n\n\n\n\n\n\n2.3.2 Stem plot\nA stem plot, also known as a stem-and-leaf plot (or just stemplot), is a graphical representation of data where each data point is split into a “stem” (the leading digit or digits) and “leaves” (the trailing digits) to display the individual values in a dataset while preserving their relative order, making it easier to see the distribution and identify key data points. Let’s create a stemplot for our chicken weight data from above.\n\n\nCode\n# Stemplot of group 1 weights\nstem(group1Weights)\n\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   2 | 599\n   4 | 011111111112222223334578889999999901111112344556667788999\n   6 | 001122233445557777888801111122234446799\n   8 | 112344445788999901233366678889\n  10 | 0011233666780222355679\n  12 | 00234455683456889\n  14 | 112468945777\n  16 | 0002234481457\n  18 | 124577257899\n  20 | 255958\n  22 | 037\n  24 | 809\n  26 | 6\n  28 | 8\n  30 | 5\n\n\nCode\n# Stemplot of group 4 weights\nstem(group4Weights)\n\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   2 | 9\n   4 | 0011122229001123345\n   6 | 122345667989\n   8 | 024455668\n  10 | 0133345878\n  12 | 02345678158\n  14 | 14567823455677\n  16 | 068034455\n  18 | 44458677899\n  20 | 03445500\n  22 | 2134478\n  24 | \n  26 | 1449\n  28 | 1\n  30 | 3\n  32 | 2\n\n\n\n\n2.3.3 Scatter Plot\nA scatter plot is a graphical representation that displays individual data points on a two-dimensional plane, with one variable on the x-axis and another on the y-axis, allowing you to visualize the relationship, pattern, or correlation between the two variables.\n\n\nCode\n# Sample data\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 3, 5, 4, 6)\n\n# Create scatter plot\nplot(x, y, main = \"Scatter Plot Example\", xlab = \"X-axis\", ylab = \"Y-axis\")\n\n\n\n\n\nReal Data Example Let’s create a scatter plot using the R command plot() for the US airline passengers by month using our data from above.\n\n\nCode\n# Airline passengers:\n# Load from the built-in dataset. This will create a variable named AirPassengers \n# containing the time series.\ndata(\"AirPassengers\")\n\n# Plot each column against the row index (year). type=\"p\" for points. \nplot(AirPassengers, main = \"US airline passengers by month\", xlab = \"Time\", \n     ylab = \"Total Passengers\", type = \"p\")\n\n\n\n\n\nNotice the overall increasing trend of the data.\n\n\n2.3.4 Time-series Graph\nA time series is a sequence of data points collected or recorded at successive points in time, typically at evenly spaced intervals, and a time series graph visually represents this data over time, allowing us to observe trends, patterns, and changes in the data’s behavior. Let’s use the R command ts.plot() to plot the total US airline passengers by month using our data from above.\n\n\nCode\n# Time series plot of AirPassengers\nts.plot(AirPassengers, main = \"US airline passengers by month\", xlab = \"Time\", \n        ylab = \"Total Passengers\")\n\n\n\n\n\nThe time series graph shows several interesting phenomena: 1) airline travel is seasonal with the same basic pattern repeated each year and 2) the overall trend is increasing.\n\n\n2.3.5 Pie Chart\nA pie chart is a circular graph that visually represents data as slices, with each slice showing the proportion or percentage of different categories in the whole dataset.\nA pie chart can be easily created as in the followng example:\n\n\nCode\n# Creating sample data\ndata &lt;- c(30, 20, 50) # Example data for the pie chart\nlabels &lt;- c(\"Category A\", \"Category B\", \"Category C\") # Labels for each category\n\n# Creating a pie chart\npie(data, labels = labels, main = \"Pie Chart Example\")\n\n\n\n\n\nReal Data Example\nLet’s use a pie chart to visualize the difference between average personal expenditure in the US in 1940 vs 1960 using USPeronalExpenditure defined above.\n\n\nCode\n# Personal expenditure:\n# Load from the built-in dataset.  This will create a variable named \n# USPersonalExpenditure containing the data.\ndata(\"USPersonalExpenditure\")\n\n# We now extract only information from 1940\nexpenditures1940 &lt;- USPersonalExpenditure[1:5]\n\n# We now extract only information from 1960\nexpenditures1960 &lt;- USPersonalExpenditure[21:25]\n\n# Define categories for expenditure data\ncats &lt;- c(\"Food and Tobacco\", \"Household Operation\", \"Medical and Health\", \n          \"Personal Care\", \"Private Education\")\n\n# Define category names from cats above\nnames(expenditures1940) &lt;- cats\nnames(expenditures1960) &lt;- cats\n\n# Pie chart of 1940 expenditures: labels allows us to name the categories as \n# defined in cats above\npie(expenditures1940, main = \"US personal expenditures in 1940\")\n\n\n\n\n\nCode\n# Pie chart of 1960 expenditures: labels allows us to name the categories as \n# defined in cats above\npie(expenditures1960, main = \"US personal expenditures in 1960\")\n\n\n\n\n\n\n\n2.3.6 Pareto Chart\nA Pareto chart is a specialized bar chart that displays data in descending order of frequency or importance, highlighting the most significant factors or categories, making it a visual tool for prioritization and decision-making. Let’s use the expenditures1940 and expenditures1960 data from above to illustrate the usefulness of a Pareto chart.\nThe first time you run this code, you will need to install the following package. After this initial run, you can skip running this code:\n\n\nCode\n# Installs the package 'qcc'.  ONLY RUN THIS CODE ONCE!\ninstall.packages('qcc')\n\n\nNow, let’s create Pareto charts for the 1940 and 1960 expenditure data.\n\n\nCode\n# Load 'qcc' package\nlibrary(qcc)\n\n\nPackage 'qcc' version 2.7\n\n\nType 'citation(\"qcc\")' for citing this R package in publications.\n\n\nCode\n# Create the Pareto chart for 1940 data \npareto.chart(expenditures1940, xlab = \"\", ylab=\"Frequency\", \n             main = \"US personal expenditures in 1940\")\n\n\n\n\n\n                     \nPareto chart analysis for expenditures1940\n                        Frequency   Cum.Freq.  Percentage Cum.Percent.\n  Food and Tobacco     22.2000000  22.2000000  59.0252852   59.0252852\n  Household Operation  10.5000000  32.7000000  27.9173646   86.9426498\n  Medical and Health    3.5300000  36.2300000   9.3855521   96.3282019\n  Personal Care         1.0400000  37.2700000   2.7651485   99.0933503\n  Private Education     0.3410000  37.6110000   0.9066497  100.0000000\n\n\nCode\n# Create the Pareto chart for 1960 data \npareto.chart(expenditures1960, xlab = \"\", ylab=\"Frequency\", \n             main = \"US personal expenditures in 1960\")\n\n\n\n\n\n                     \nPareto chart analysis for expenditures1960\n                       Frequency  Cum.Freq. Percentage Cum.Percent.\n  Food and Tobacco     86.800000  86.800000  53.205835    53.205835\n  Household Operation  46.200000 133.000000  28.319235    81.525070\n  Medical and Health   21.100000 154.100000  12.933677    94.458747\n  Personal Care         5.400000 159.500000   3.310040    97.768788\n  Private Education     3.640000 163.140000   2.231212   100.000000\n\n\n\n\n2.3.7 Let’s put it all together!\nUsing the built-in dataset for quarterly profits of the company Johnson & Johnson, load the data and view it using this code.\n\n\nCode\n# Johnson & Johnson Profits:\n# Load data from the built-in dataset into a variable named JohnsonJohnson\ndata(\"JohnsonJohnson\")\n\nJohnsonJohnson\n\n\n      Qtr1  Qtr2  Qtr3  Qtr4\n1960  0.71  0.63  0.85  0.44\n1961  0.61  0.69  0.92  0.55\n1962  0.72  0.77  0.92  0.60\n1963  0.83  0.80  1.00  0.77\n1964  0.92  1.00  1.24  1.00\n1965  1.16  1.30  1.45  1.25\n1966  1.26  1.38  1.86  1.56\n1967  1.53  1.59  1.83  1.86\n1968  1.53  2.07  2.34  2.25\n1969  2.16  2.43  2.70  2.25\n1970  2.79  3.42  3.69  3.60\n1971  3.60  4.32  4.32  4.05\n1972  4.86  5.04  5.04  4.41\n1973  5.58  5.85  6.57  5.31\n1974  6.03  6.39  6.93  5.85\n1975  6.93  7.74  7.83  6.12\n1976  7.74  8.91  8.28  6.84\n1977  9.54 10.26  9.54  8.73\n1978 11.88 12.06 12.15  8.91\n1979 14.04 12.96 14.85  9.99\n1980 16.20 14.67 16.02 11.61\n\n\nNow, select the best plot from those illustrated above and plot this data. Hint: this looks like a time series to me…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "ch2.html#scatter-plots-correlation-and-regression",
    "href": "ch2.html#scatter-plots-correlation-and-regression",
    "title": "2  Exploring Data with Tables and Graphs",
    "section": "2.4 Scatter plots, correlation, and regression",
    "text": "2.4 Scatter plots, correlation, and regression\nCorrelation quantifies the strength and direction of the relationship between two variables, helping assess how they move together (or in opposite directions). Any potential such relationship can be visualized using a scatter plot as introduced in Section 2.3.\n\n2.4.1 Linear correlation\nLinear correlation measures the strength and direction of the linear relationship between two variables, often represented by the correlation coefficient (r). The p-value associated with this coefficient assesses the statistical significance of the correlation, helping determine whether the observed relationship is likely due to chance or represents a real association. Let’ consider the built-in dataset mtcars which contains several aspects and performance of several 1973 - 1974 model cars. This code loads the dataset and displays several of its entries.\n\n\nCode\n# mtcars:\n# Load data from the built-in dataset into a variable named mtcars\ndata(\"mtcars\")\n\nmtcars\n\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nLet’s see if there is a linear relationship between miles per gallon (MPG) and the engine horse powerr (HP) using the R command cor.test() and storing the linear correlation coefficient (r) and P-value in the variable mpgvshp. Notice that mtcars$mpg extracts just the column of MPG from the dataset and similarly for mtcars$hp. The r-value can be found by calling mpgvshp$estimate, whereas, the P-value can be found by calling mpgvshp$p.value. Finally, the confidence interval for the estimated r is found using the mpgvshp$conf.int command.\n\n\nCode\n# Calculate the correlation between MPG and HP\nmpgvshp &lt;- cor.test(mtcars$mpg, mtcars$hp)\nmpgvshp\n\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$mpg and mtcars$hp\nt = -6.7424, df = 30, p-value = 1.788e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.8852686 -0.5860994\nsample estimates:\n       cor \n-0.7761684 \n\n\nCode\n# Let's view the r- and P-values and critical r-value range\ncat(\"r:\", mpgvshp$estimate, \"\\n\")\n\n\nr: -0.7761684 \n\n\nCode\ncat(\"P-value:\", mpgvshp$p.value, \"\\n\")\n\n\nP-value: 1.787835e-07 \n\n\nCode\ncat(\"Confidence interval for r: (\", mpgvshp$conf.int[1], \", \", mpgvshp$conf.int[2], \")\")\n\n\nConfidence interval for r: ( -0.8852686 ,  -0.5860994 )\n\n\nA negative r-value indicates that if a linear relationship is present then the relationship is negative, i.e., increasing the MPG decreases the HP. Having the absolute value of the r-value close to one indicates a linear relationship. Notice that the confidence interval for r is away from zero, supporting the conclusion that a negative linear relationship is present.\nA P-value of less than 0.05 suggests that the sample results are not likely to occur merely by chance when there is no linear correlation. Thus, a small P-value such as the one we received here supports a conclusion that there is a linear correlation between MPG and HP.\nNow, let’s use a scatter plot to visualize the relationship.\n\n\nCode\n# Create a scatter plot to visualize the relationship\nplot(mtcars$mpg, mtcars$hp, xlab = \"Miles per Gallon (MPG)\", ylab = \"Horsepower (HP)\", \n     main = \"Plot of MPG vs. HP\")\n\n\n\n\n\n\n\n2.4.2 Regression line\nRegression analyzes and models the relationship between variables, allowing us to predict one variable based on the values of others. Let’s return to our MPG vs HP example. We will use the R command lm() to create a linear model (or linear regression) for this data. We then use our scatter plot created previously to plot the model prediction alongside the actual data points. In this case, the R command abline() adds the regression line stored in model with the color being specified by the attribute col.\n\n\nCode\n# Create a linear regression model\nmodel &lt;- lm(hp ~ mpg, data = mtcars)\n\n# Create a scatter plot to visualize the relationship\nplot(mtcars$mpg, mtcars$hp, xlab = \"Miles per Gallon (MPG)\", ylab = \"Horsepower (HP)\", \n      main = \"Plot of MPG vs. HP\")\n\n# Add the regression line to the plot\nabline(model, col = \"blue\")\n\n\n\n\n\n\n\n2.4.3 Let’s put it all together!\nUsing the same mtcars dataset, use what you have learned above to determine if there is a linear correlation between the weight of a car in the set versus the engine’s horse power. The following code will walk you through the process. We begin with a visualization of the data using a scatter plot.\n\n\nCode\n# Create a scatter plot to visualize the relationship\nplot(mtcars$wt, mtcars$hp, xlab = \"Weight (WT)\", ylab = \"Horsepower (HP)\", \n     main = \"Plot of WT vs. HP\")\n\n\n\n\n\nNow, let’s determine if there is a linear relationship between car weight mtcars$wt and engine horsepower mtcars$hp.\n\n\nCode\n# Calculate the correlation between MPG and HP\nwtvshp &lt;- cor.test(mtcars$wt, mtcars$hp)\n\nwtvshp\n\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$wt and mtcars$hp\nt = 4.7957, df = 30, p-value = 4.146e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4025113 0.8192573\nsample estimates:\n      cor \n0.6587479 \n\n\nCode\n# Let's view the r- and P-values and critical r-value range\ncat(\"r:\", wtvshp$estimate, \"\\n\")\n\n\nr: 0.6587479 \n\n\nCode\ncat(\"P-value:\", wtvshp$p.value, \"\\n\")\n\n\nP-value: 4.145827e-05 \n\n\nCode\ncat(\"Confidence interval for r: (\", wtvshp$conf.int[1], \", \", wtvshp$conf.int[2], \")\")\n\n\nConfidence interval for r: ( 0.4025113 ,  0.8192573 )\n\n\nWhat can we conclude about a possible linear relationship between car weight and horsepower? Is this relationship supported? Finally, let’s visualize the regression line and data together.\n\n\nCode\n# Create a linear regression model\nmodel2 &lt;- lm(hp ~ wt, data = mtcars)\n\n# Create a scatter plot to visualize the relationship\nplot(mtcars$wt, mtcars$hp, xlab = \"Weight (WT)\", ylab = \"Horsepower (HP)\", \n     main = \"Plot of WT vs. HP\")\n\n# Add the regression line to the plot\nabline(model2, col = \"red\")\n\n\n\n\n\nWhat about causation? Does having a heavier car make it have higher or lower horsepower?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Data with Tables and Graphs</span>"
    ]
  },
  {
    "objectID": "ch3.html#measures-of-center",
    "href": "ch3.html#measures-of-center",
    "title": "3  Describing, Exploring, and Comparing Data",
    "section": "3.1 Measures of center",
    "text": "3.1 Measures of center\nMeasures of center, such as the mean and median, provide a central value that summarizes a dataset, helping to understand its typical or central tendency, which is crucial for making data-driven decisions and drawing inferences.\n\n3.1.1 Mean\nThe mean, also known as the average, is a measure of center in a dataset that calculates the sum of all values divided by the total number of values, providing a representative value for the dataset. We will employ the R command mean() to calculate the mean of several datasets.\n\n\nCode\n# Load test data into a variable names scores\nscores &lt;- c(95, 90, 85, 85, 87, 74, 75, 64, 85, 84, 87, 15, 20, 75, 75, 90, 75)\n# Calculate mean of scores and then store it in the variable meanScore\nmeanScore &lt;- mean(scores)\n\n# print out the answer\ncat(\"Mean test score is: \", meanScore, \"\\n\")\n\n\nMean test score is:  74.17647 \n\n\nThe mean is very sensitive to outliers. Let’s see what happens when we take the same scores list and add some really low grades to the list.\n\n\nCode\n# Previous test scores with a several much lower scores added\nscores2 &lt;- c(95, 90, 85, 85, 87, 74, 75, 64, 85, 84, 87, 15, 20, 75, 75, 90, 75, \n             2, 1, 5, 3)\n\n# Calculate mean of scores2 and then store it in the variable meanScore2\nmeanScore2 &lt;- mean(scores2)\n\n# print out the answer\ncat(\"Mean test score is from original is: \", meanScore, \", while from scores2 is: \", \n    meanScore2)\n\n\nMean test score is from original is:  74.17647 , while from scores2 is:  60.57143\n\n\nThis sensitivity to outliers is the notion of resistance. The mean is not a resistant measure of middle.\n\n\n3.1.2 Median\nThe median is a measure of center in a dataset that represents the middle value when all values are ordered, and it is resistant to extreme outliers, making it a robust statistic for summarizing data. Let’s return to the scores data and see the difference between mean and median of the two datasets scores and scores2 using the R commands median().\n\n\nCode\n# Calculate median of scores and then store it in the variable medianScore\nmedianScore &lt;- median(scores)\n\n# Calculate median of scores2 and then store it in the variable medianScore2\nmedianScore2 &lt;- median(scores2)\n\n# print out the answer\ncat(\"Mean test score from original is: \", meanScore, \", while from scores2 is: \", \n    meanScore2, \"\\n\\n\")\n\n\nMean test score from original is:  74.17647 , while from scores2 is:  60.57143 \n\n\nCode\ncat(\"Median test score from original is: \", medianScore, \", while from scores2 is: \", medianScore2, \"\\n\")\n\n\nMedian test score from original is:  84 , while from scores2 is:  75 \n\n\n\n\n3.1.3 Mode\nThe mode is a statistical measure that represents the value or values that occur most frequently in a dataset, making it a useful indicator of the most common observation(s); however, it is not necessarily resistant to outliers, meaning extreme values can heavily influence the mode. There is no bulit-in R command for mode, so we will have to employ the package DescTools.\nThe first time you run this code, you will need to install the following package. After this initial run, you can skip running this code:\n\n\nCode\n# Installs the package 'DescTools'.  ONLY RUN THIS CODE ONCE!\ninstall.packages('DescTools')\n\n\nOnce this package is installed, then we can load the library DescTools in order to use the R command Mode().\n\n\nCode\n# Load the DescTools package\nlibrary(DescTools)\n\n# Calculate the mode of both scores and scores2 using the Mode() method\n\n# Calculate Mode of scores and then store it in the variable modeScore\nmodeScore &lt;- Mode(scores)\n\n# Calculate median of scores2 and then store it in the variable modeScore2\nmodeScore2 &lt;- Mode(scores2)\n\n# print out the answer\ncat(\"Mode test score from original is: \", modeScore, \", while from scores2 is: \", \n    modeScore2, \"\\n\")\n\n\nMode test score from original is:  75 , while from scores2 is:  75 \n\n\n\n\n3.1.4 Midrange\nThe midrange is a measure of center in a dataset that represents the arithmetic mean of the maximum and minimum values, and it is not resistant to extreme outliers, making it sensitive to extreme values. There is no built-in R command for midrange, thus we will use the following code to calculate the midrange of our scores and scores2 data.\n\n\nCode\n# Calculate miderange of scores and then store it in the variable midrangeScore\nmidrangeScore &lt;- (max(scores) - min(scores)) / 2\n\n# Calculate midrange of scores2 and then store it in the variable midrangeScore2\nmidrangeScore2 &lt;- (max(scores2) - min(scores2)) / 2\n\n# print out the answer\ncat(\"Midrange test score from original is: \", midrangeScore, \", \n    while from scores2 is: \", midrangeScore2, \"\\n\")\n\n\nMidrange test score from original is:  40 , \n    while from scores2 is:  47 \n\n\n\n\n3.1.5 Let’s put it all togeher!\nConsider the built-in dataset mtcars which contains several aspects and performance of several 1973 - 1974 model cars which we studied in Section 2.4. We will calculate mean, meidan, mode, and midrange of the miles per gallon of the cars in the dataset. using the R commands illustrated in the previous sections, as well as compute the so-called 5-number summary using the R command summary(). First, let’s plot a histogram of the data.\n\n\nCode\n# Extract the MPG data and store it into the variable carsMPG\ncarsMPG &lt;- mtcars$mpg\n\n# Generate a histogram of the MPG data from mtcars\nhist(carsMPG, main = \"MPG for cars\", xlab = \"MPG\")\n\n\n\n\n\nCode\n# Calculate mean of MPG data and then store it in the variable meanMPG\nmeanMPG &lt;- round(mean(carsMPG), digits = 2)\n\n# Calculate median of MPG data and then store it in the variable medianMPG\nmedianMPG &lt;- median(carsMPG)\n\n# Calculate Mode of scores and then store it in the variable modeMPG\nmodeMPG &lt;- Mode(carsMPG)\n\n# Calculate miderange of scores and then store it in the variable midrangeMPG\nmidrangeMPG &lt;- (max(carsMPG) - min(carsMPG)) / 2\n\n# print out the answer\ncat(\"Mean \\t Median \\t \\t \\t  Mode \\t \\t \\t Midrange \\n\")\n\n\nMean     Median               Mode           Midrange \n\n\nCode\ncat(meanMPG, \" \\t \", medianMPG, \" \\t \", modeMPG, \" \\t \", midrangeMPG, \"\\n\")\n\n\n20.09     19.2        10.4 15.2 19.2 21 21.4 22.8 30.4        11.75 \n\n\nCode\n# Give the 5-number summary for MPG data\nsummary(carsMPG)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\n\nNotice that there are 7 elements in the mode. That’s because there are 7 most frequent elements, each appear twice. Which of these central measures best describes what you visually see as the “center” of data using the histogram? What does it “mean” that the mean and median are close to each other? Does the 5-number summary give us any additional information regarding the measure of “center” in the data?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing, Exploring, and Comparing Data</span>"
    ]
  },
  {
    "objectID": "ch3.html#measures-of-variation",
    "href": "ch3.html#measures-of-variation",
    "title": "3  Describing, Exploring, and Comparing Data",
    "section": "3.2 Measures of variation",
    "text": "3.2 Measures of variation\nMeasures of variation, such as the range, variance, and standard deviation, provide insights into the spread or dispersion of data points within a dataset, helping us understand how much individual values deviate from the central tendency measures like the mean or median. These measures are essential because they quantify the degree of variability in data, allowing us to assess data quality, make more accurate predictions, and draw meaningful conclusions in statistical analysis.\n\n3.2.1 Visualizing variation\nHistograms can visually represent the variation in a dataset by displaying the distribution of values across different bins or intervals, highlighting the frequency and pattern of data points, and revealing the shape and spread of the distribution. Let’s compare histograms for our scores2 and carsMPG datasets.\n\n\nCode\n# Generate a histogram of the MPG data from scores2\nhist(scores2, main = \"Test scores\", xlab = \"Score\")\n\n\n\n\n\nCode\n# Generate a histogram of the MPG data from mtcars\nhist(carsMPG, main = \"MPG for cars\", xlab = \"MPG\")\n\n\n\n\n\n\n\n3.2.2 Range\nThe range is a measure of variation that represents the difference between the maximum and minimum values in a dataset, but it is not resistant to outliers, meaning extreme values can substantially affect the range. Let’s compare the ranges of our carsMPG and scores2 datasets using the R command range().\n\n\nCode\n# Calculate range of scores2 and then store it in the variable rangeScore2\nrangeScore2 &lt;- range(scores2)\n\n# Calculate range of carsMPG and then store it in the variable rangeMPG\nrangeMPG &lt;- range(carsMPG)\n\n# print out the answer\ncat(\"Range for test scores from scores2 is: (\", rangeScore2[1], \", \",\n    rangeScore2[2], \") \\n\")\n\n\nRange for test scores from scores2 is: ( 1 ,  95 ) \n\n\nCode\ncat(\"Range for MPG from carsMPG is: (\", rangeMPG[1], \", \", rangeMPG[2], \") \\n\")\n\n\nRange for MPG from carsMPG is: ( 10.4 ,  33.9 ) \n\n\n\n\n3.2.3 Standard deviation\nStandard deviation is a measure of the dispersion or spread of data points in a dataset, with a higher value indicating greater variability, and it’s calculated differently for populations (\\sigma) and samples (s), where the sample standard deviation (s) is often used for practical data analysis. However, standard deviation is not resistant to extreme outliers, making it sensitive to the influence of extreme values on its magnitude. There is a built-in R command for sample standard deviation, but no such command for population standard deviation. Recall our test scores dataset scores2. Since this data represents the entire population (every student in the class), we will calculate population standard deviation for that dataset. However, the MPG data in carsMPG is only a sample of all the cars on the market in 1973 - 1974. Thus, we will employ the R command sd() to calculate sample standard deviation.\n\n\nCode\n# Calculate population SD of scores2 and then store it in the variable popSDScore2\npopSDScore2 &lt;- sqrt(var(scores2) * (length(scores2) - 1) / length(scores2))\n\n# Calculate sample SD of carsMPG and then store it in the variable samSDMPG\nsamSDMPG &lt;- sd(carsMPG)\n\n# Print out the answer\ncat(\"Population standard deviation for test scores from scores2 is: \", popSDScore2, \"\\n\\n\")\n\n\nPopulation standard deviation for test scores from scores2 is:  34.35331 \n\n\nCode\ncat(\"Sample standard deviation for MPG from carsMPG is: \", samSDMPG, \" \\n\")\n\n\nSample standard deviation for MPG from carsMPG is:  6.026948  \n\n\n\n\n3.2.4 Variance\nVariance measures the average of the squared differences between each data point and the mean of a dataset, providing a measure of data dispersion, but it is not resistant to extreme outliers, making it sensitive to the influence of extreme values on its magnitude. Variance is calculated differently for populations (\\sigma^2) and samples (s^2), with the sample variance (s^2) being used for practical data analysis to account for bias when working with a subset of a larger population. Let’s compare population variance for our scores2 dataset and sample variance for our carsMPG dataset. As with standard deviation, although there is a built-in R command for sample variance, there is not a built-in command for population variance, so we will have to improvise.\n\n\nCode\n# Calculate population variance of scores2 and then store it in the variable \n# popVarScore2\npopVarScore2 &lt;- var(scores2) * (length(scores2) - 1) / length(scores2)\n\n# Calculate sample variance of carsMPG and then store it in the variable samSDMPG\nsamVarMPG &lt;- var(carsMPG)\n\n# Print out the answer\ncat(\"Population variance for test scores from scores2 is: \", popVarScore2, \"\\n\\n\")\n\n\nPopulation variance for test scores from scores2 is:  1180.15 \n\n\nCode\ncat(\"Sample variance for MPG from carsMPG is: \", samVarMPG, \" \\n\")\n\n\nSample variance for MPG from carsMPG is:  36.3241  \n\n\n\n\n3.2.5 Let’s put it all togeher!\nConsider the built-in dataset mtcars which contains several aspects and performance of several 1973 - 1974 model cars which we studied in Section 2.4. We will first calculate mean and median of the horse power (HP) of the cars in the dataset. To calculate measures of variation, we note that since this is just a sample of all possible cars on the market during 1973 - 1974, we will employ sample variance and standard deviation using the R commands illustrated in the previous sections, along with a histogram to visually explore the data.\n\n\nCode\n# Extract the HP data and store it into the variable carsHP\ncarsHP &lt;- mtcars$hp\n\n# Generate a histogram of the HP data from mtcars\nhist(carsHP, main = \"Horse power (HP) for cars\", xlab = \"HP\")\n\n\n\n\n\nCode\n# Calculate mean of HP data and then store it in the variable meanHP\nmeanHP &lt;- round(mean(carsHP), digits = 2)\n\n# Calculate median of HP data and then store it in the variable medianHP\nmedianHP &lt;- median(carsHP)\n\n# Calculate variance of HP data and then store it in the variable varHP\nvarHP &lt;- var(carsHP)\n\n# Calculate standard deviation of HP and then store it in the variable midrangeHP\nsdHP &lt;- sd(carsHP)\n\n# print out the answer\ncat(\"Mean \\t Median \\t  variance \\t Standard Deviation \\n\")\n\n\nMean     Median       variance   Standard Deviation \n\n\nCode\ncat(meanHP, \" \\t \", medianHP, \" \\t \", varHP, \" \\t \", sdHP, \"\\n\")\n\n\n146.69        123     4700.867        68.56287 \n\n\nNow, compare the MPG and HP data from the mtcars dataset. For MPG, we calculated a standard deviation around 36 and for HP of around 69. Does this mean that the MPG data is less spread out than the HP data? Is your answer to this question consistent with the histograms we produced? Can we compare standard deviations from two totally different datasets in a meaningful way?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing, Exploring, and Comparing Data</span>"
    ]
  },
  {
    "objectID": "ch3.html#measures-of-relative-standing-and-boxplots",
    "href": "ch3.html#measures-of-relative-standing-and-boxplots",
    "title": "3  Describing, Exploring, and Comparing Data",
    "section": "3.3 Measures of relative standing and boxplots",
    "text": "3.3 Measures of relative standing and boxplots\nMeasures of relative standing, such as percentiles and quartiles, provide information about where specific data points fall within a dataset, offering insights into the relative position of values. Boxplots are graphical representations that display the distribution of data, highlighting the median, quartiles, and potential outliers, making them valuable tools for comparing different datasets by visually assessing their central tendency, spread, and skewness.\n\n3.3.1 z-Scores\nZ-scores, also known as standard scores, standardize individual data points by expressing how many standard deviations they are from the mean, enabling meaningful comparisons and assessments of data points’ relative positions within a distribution, regardless of the original scale of the data. Z-scores are valuable for identifying outliers, understanding data distributions, and making statistical inferences, as they provide a common framework for measuring deviations from the mean across different datasets. Let’s explore z-scores using the built-in dataset mtcars which contains several aspects and performance of several 1973 - 1974 model cars which we studied in the last section. Particularly, let’s employ the built-in R command scale() to convert our dataset to z-scores which can be plotted in a histogram. Once the two datasets (MPG and HP) are normalized, we will be able to get a better picture of their spread away from the respective means.\n\n\nCode\n# Transform the MPG data to z-scores and store the new data in zcarsHP\nzcarsHP &lt;- scale(carsHP)\n\n# Transform the MPG data to z-scores and store the new data in zcarsHP\nzcarsMPG &lt;- scale(carsMPG)\n\n# Generate a histogram of the transformed HP data from mtcars\nhist(zcarsHP, main = \"Normalized horse power (HP) for cars\", xlab = \"Z-score\")\n\n\n\n\n\nCode\n# Generate a histogram of the transformed MPG data from mtcars\nhist(zcarsMPG, main = \"Normalized miles per gallon (MPG) for cars\", xlab = \"Z-score\")\n\n\n\n\n\nVisually, the normalized MPG data is more concentrated around the transformed mean of 0, while the HP data is much more spread out.\nAny data point that has a z-score of less than -2 or higher than 2 is considered to be significantly lower or higher, respectively. Let’s view our transformed data sets MPG and HP to identify data points that are significantly higher.\n\n\nCode\n# Find MPG data points with z-scores higher than 2\noutliersMPG &lt;- carsMPG[zcarsMPG &gt; 2]\n\n# Find HP data points with z-scores higher than 2\noutliersHP &lt;- carsHP[zcarsHP &gt; 2]\n\n# Print the data points with z-scores higher than 2\ncat(\"MPG Data with z-scores higher than 2:\", outliersMPG, \"\\n\")\n\n\nMPG Data with z-scores higher than 2: 32.4 33.9 \n\n\nCode\ncat(\"HP Data with z-scores higher than 2:\", outliersHP, \"\\n\")\n\n\nHP Data with z-scores higher than 2: 335 \n\n\n\n\n3.3.2 Percentiles\nPercentiles are statistical measures that divide a dataset into 100 equal parts, helping identify values below which a certain percentage of the data falls and enabling comparisons of data points in a ranked order. Let’s use the built-in R command quantile()with the MPG data from the previous example to compute the 10th, 50th, and 90th percentiles for that dataset.\n\n\nCode\n# Compute 10th, 50th, and 90th percentiles for the MPG dataset\npercentiles &lt;- c(0.1, 0.5, 0.9)\npercentilesMPG &lt;- quantile(carsMPG, probs = percentiles)\n\n# Print the data points with z-scores higher than 2\npercentilesMPG\n\n\n  10%   50%   90% \n14.34 19.20 30.09 \n\n\nNotice that both of our significantly larger MPG values (i.e., 32.4 and 33.9) both fall above the 90th percentile of the dataset.\n\n\n3.3.3 Quartiles & the 5-number summary\nQuartiles are statistical measures that divide a dataset into four equal parts, with three quartiles (Q1, Q2, Q3) providing insights into the data’s spread and central tendencies; they are resistant to outliers, making them robust tools for summarizing data. The 5-number summary is a set of five statistics (minimum, Q1, median, Q3, maximum) that provide a concise description of a dataset’s central tendencies and spread. Keeping with our MPG dataset, we will employ the R command summary() to give the 5-number summary (which will include Q1, Q2 (also known as the median), & Q3).\n\n\nCode\n# Compute 5-number summary for MPG data and store it in fiveMPG\nfiveMPG &lt;- summary(carsMPG)\n\n# Display the 5-number summary\nfiveMPG\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\n\n\n\n3.3.4 Boxplot\nA boxplot, also known as a box-and-whisker plot, is a graphical representation of the five-number summary, displaying the median, quartiles, and potential outliers in a dataset, making it a valuable tool for visualizing the distribution and spread of data. We will employ the R command boxplot() to compare the MPG and HP datasets from previous examples. This R command actually creates a modified boxplot by default. Recall the only difference between a regular boxplot and a modified box plot is that data which falls outside of the interquartile range is denoted as an outlier and plotted as an individual point on the graph.\n\n\nCode\n# Generate boxplot for MPG\nboxplot(carsMPG, main = \"Boxplot of MPG\", horizontal = TRUE, xlab = \"MPG\")\n\n\n\n\n\nCode\n# Generate boxplot for HP\nboxplot(carsHP, main = \"Boxplot of HP\", horizontal = TRUE, xlab = \"HP\")\n\n\n\n\n\nLet’s also compare the boxplots of each of the four datasets for which we explored normal, skewed right, skewed left, and uniform distributions.\n\n\nCode\n# Create histogram/boxplot of normal data\n# Sample normal distribution\nnormalData &lt;- rnorm(100)\nhist(normalData, main = \"Normal distribution\")\n\n\n\n\n\nCode\nboxplot(normalData, main = \"Normal Distribution\", horizontal = TRUE)\n\n\n\n\n\nCode\n# Create histogram/boxplot of uniform data\n# Sample uniform distribution using the command runif\nuniformData &lt;- runif(50000, min = 10, max = 11)\nhist(uniformData, main = \"Uniform distribution\")\n\n\n\n\n\nCode\nboxplot(uniformData, main = \"Uniform Distribution\", horizontal = TRUE)\n\n\n\n\n\nCode\n# Sample of a distribution that is skewed right\nskewedRightData &lt;- rexp(1000, 0.4)\n# Create histogram/boxplot of skewed right data\nhist(skewedRightData, main = \"Distribution that is skewed right\")\n\n\n\n\n\nCode\nboxplot(skewedRightData, main = \"Distribution that is skewed right\", horizontal = TRUE)\n\n\n\n\n\nCode\n# Sample of a distribution that is skewed left\nskewedLeftData &lt;- 1 - rexp(1000, 0.2)\n# Create histogram/boxplot of skewed left data\nhist(skewedLeftData, main = \"Distribution that is skewed left\")\n\n\n\n\n\nCode\nboxplot(skewedLeftData, main = \"Distribution that is skewed left\", horizontal = TRUE)\n\n\n\n\n\nNotice there are a lot of outliers shown on the skewed left & right data. These points are what is causing the long tails on both histograms.\n\n\n3.3.5 Let’s put it all together!\nWe will use everything we have learned so far in this section to explore the differences between our two test score datasets, i.e., scores and scores2. These are fictional collections of test scores with scores2 containing several more extremely low test scores than scores. Our first task is to transform the datasets to z-scores and visualize the scaled datasets with a histrogram.\n\n\nCode\n# Transform the scores data to z-scores and store the new data in zscores\nzscores &lt;- scale(scores)\n\n# Transform the scorres2 data to z-scores and store the new data in zscores2\nzscores2 &lt;- scale(scores2)\n\n# Generate a histogram of the transformed from scores\nhist(zscores, main = \"Normalized test scores #1\", xlab = \"Z-score\")\n\n\n\n\n\nCode\n# Generate a histogram of the transformed from scores2\nhist(zscores2, main = \"Normalized test scores #2\", xlab = \"Z-score\")\n\n\n\n\n\nOut of the two fictional classes, are there any test scores that are significantly high or low? What can we conclude about those scores? Now, let’s compute the 5-number summary for each group of test scores.\n\n\nCode\n# Compute 5-number summary for scores\nsummary(scores)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.00   75.00   84.00   74.18   87.00   95.00 \n\n\nCode\n# Compute 5-number summary for scores2\nsummary(scores2)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   20.00   75.00   60.57   85.00   95.00 \n\n\nFinally, let’s create boxplots for both datasets and show them on the same plot window for comparison.\n\n\nCode\n# Generate boxplot for both\nboxplot(scores, main = \"Boxplot of test scores #1\", horizontal = TRUE, xlab = \"Scores\")\n\n\n\n\n\nCode\nboxplot(scores2, main = \"Boxplot of test scores #2\", horizontal = TRUE, xlab = \"Scores\")\n\n\n\n\n\nWhat conclusions can we draw regarding the two datasets? If these were two real classes, how would the boxplots help the teacher understand grade performance for the entire class?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Describing, Exploring, and Comparing Data</span>"
    ]
  },
  {
    "objectID": "ch4.html#basic-concepts-of-probability",
    "href": "ch4.html#basic-concepts-of-probability",
    "title": "4  PROBABILITY",
    "section": "4.1 Basic Concepts of Probability",
    "text": "4.1 Basic Concepts of Probability\nIn this code:\n\nWe calculate the probability of drawing a Heart from a deck of four suits (sample space).\nWe simulate random events such as a coin toss and rolling a six-sided die.\nWe simulate multiple die rolls and visualize the resulting probability distribution.\nWe calculate the probability of a specific outcome (such as, rolling a 3).\n\n\n\nCode\n# Set a seed for reproducibility\nset.seed(42)\n\n# Define a sample space (e.g., a deck of cards)\nsample_space &lt;- c(\"Hearts\", \"Diamonds\", \"Clubs\", \"Spades\")\n\n# Calculate the probability of drawing a Heart from the sample space\nprobability_heart &lt;- sum(sample_space == \"Hearts\") / length(sample_space)\n\ncat(\"Probability of drawing a Heart:\", probability_heart, \"\\n\")\n\n\nProbability of drawing a Heart: 0.25 \n\n\n\n\nCode\n# Simulate a random event (e.g., coin toss)\ncoin_toss &lt;- sample(c(\"Heads\", \"Tails\"), size = 1)\n\ncat(\"Result of a random coin toss:\", coin_toss, \"\\n\")\n\n\nResult of a random coin toss: Heads \n\n\n\n\nCode\n# Simulate rolling a six-sided die\ndie_roll &lt;- sample(1:6, size = 1)\n\ncat(\"Result of rolling a die:\", die_roll, \"\\n\")\n\n\nResult of rolling a die: 5 \n\n\n\n\nCode\n# Simulate multiple die rolls and visualize the probability distribution\nnum_rolls &lt;- 1000\ndie_rolls &lt;- sample(1:6, size = num_rolls, replace = TRUE)\n\n# Calculate the relative frequencies for each outcome\nrelative_frequencies &lt;- table(die_rolls) / num_rolls\nrelative_frequencies\n\n\ndie_rolls\n    1     2     3     4     5     6 \n0.171 0.192 0.164 0.157 0.154 0.162 \n\n\nCode\n# Calculate the probability of rolling a 3\nprobability_roll_3 &lt;- relative_frequencies[3]\n\ncat(\"Probability of rolling a 3:\", probability_roll_3, \"\\n\")\n\n\nProbability of rolling a 3: 0.164 \n\n\n\n\nCode\n# Visualize the probability distribution with a bar plot\nbarplot(relative_frequencies, main = \"Probability Distribution of Die Rolls\",\n        xlab = \"Die Face\", ylab = \"Probability\", col = \"lightblue\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PROBABILITY</span>"
    ]
  },
  {
    "objectID": "ch4.html#addition-rule-and-multiplication-rule",
    "href": "ch4.html#addition-rule-and-multiplication-rule",
    "title": "4  PROBABILITY",
    "section": "4.2 Addition rule and multiplication rule",
    "text": "4.2 Addition rule and multiplication rule",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PROBABILITY</span>"
    ]
  },
  {
    "objectID": "ch4.html#complements-conditional-probability-and-bayes-theorem",
    "href": "ch4.html#complements-conditional-probability-and-bayes-theorem",
    "title": "4  PROBABILITY",
    "section": "4.3 Complements, conditional probability, and Bayes’ theorem",
    "text": "4.3 Complements, conditional probability, and Bayes’ theorem",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PROBABILITY</span>"
    ]
  },
  {
    "objectID": "ch4.html#counting",
    "href": "ch4.html#counting",
    "title": "4  PROBABILITY",
    "section": "4.4 Counting",
    "text": "4.4 Counting\n\n4.4.1 Calculate factorial n!\nR provides a built-in function to calculate factorial. You can use the factorial() function in R to compute the factorial of a number.\n\n\nCode\nn &lt;- 5\nfactorial_result &lt;- factorial(n)\ncat(\"Factorial of\", n, \"is\", factorial_result, \"\\n\")\n\n\nFactorial of 5 is 120 \n\n\nReplace the value of n with the number for which you want to calculate the factorial, and the factorial() function will return the result.\n\n\n4.4.2 Find all permutations and the number of all permutations\nTo do this, we can use the permutations function from the gtools package. For any list of size n, this function computes all the different permutations P(n,r) we can get when we select r items. Here are all the ways we can choose two numbers from a list consisting of 1,2,3:\n\n\nCode\nlibrary(gtools)\npermutations(3, 2)\n\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    2    1\n[4,]    2    3\n[5,]    3    1\n[6,]    3    2\n\n\nNotice that the order matters here: 3,1 is different than 1,3. Also, note that (1,1), (2,2), and (3,3) do not appear because once we pick a number, it can’t appear again.\nTo get the actual number of permutations, one can use the R-function nrow() to find the total number of rows in the output of permutations:\n\n\nCode\nlibrary(gtools)\nnrow(permutations(3,2))\n\n\n[1] 6\n\n\nAlternatively, we can add a vector v to indicate the objects that a permutation is performed on. If you want to see five random seven digit phone numbers out of all possible phone numbers (without repeats), you can type:\n\n\nCode\nall_phone_numbers &lt;- permutations(10, 7, v = 0:9) # Use digits 0, 1, ..., 9 \nn &lt;- nrow(all_phone_numbers)\ncat(\"total number of phone numbers n = \", n, \"\\n\")\n\n\ntotal number of phone numbers n =  604800 \n\n\nCode\n# Randomly sample 5 phone numbers\nindex &lt;- sample(n, 5)\nall_phone_numbers[index,] \n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]    8    9    5    1    6    0    3\n[2,]    4    0    2    3    5    7    8\n[3,]    0    4    6    1    3    2    7\n[4,]    5    8    2    6    4    0    3\n[5,]    7    5    1    0    9    2    6\n\n\nThe code all_phone_numbers[index,] extract the matrix all_phone_numbers with rows indexed by index, and all columns because no specific column index is given after ,.\nInstead of using the numbers 1 through 10, the default, it uses what we provided through v: the digits 0 through 9.\n\n\n4.4.3 Find all combinations and the number of all combinations\nHow about if the order doesn’t matter? For example, in Blackjack if you get an Ace and a face card in the first draw, it is called a Natural 21 and you win automatically. If we wanted to compute the probability of this happening, we would enumerate the combinations, not the permutations, since the order of drawn cards does not matter.\n\n\nCode\ncombinations(3,2)\n\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    2    3\n\n\nIn the second line, the outcome does not include (2,1) because (1,2) already was enumerated. The same applies to (3,1) and (3,2).\nTo get the actual number of combinations, one can do\n\n\nCode\nnrow(combinations(3,2))\n\n\n[1] 3\n\n\n(optional) Of course, one can define a R-function to calculate a permutation number.\n\n\nCode\n# Function to calculate permutation (nPr)\nnPr &lt;- function(n, r) {\n  if (n &lt; r) {\n    return(0)\n  } else {\n    return(factorial(n) / factorial(n - r))\n  }\n}\nnPr(3,2)\n\n\n[1] 6\n\n\nCode\n# Function to calculate combination (nCr)\nnCr &lt;- function(n, r) {\n  if (n &lt; r) {\n    return(0)\n  } else {\n    return(factorial(n) / (factorial(r) * factorial(n - r)))\n  }\n}\nnCr(3,2)\n\n\n[1] 3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PROBABILITY</span>"
    ]
  },
  {
    "objectID": "ch5.html#probability-distribution",
    "href": "ch5.html#probability-distribution",
    "title": "5  Discrete Probability Distribution",
    "section": "5.1 Probability Distribution",
    "text": "5.1 Probability Distribution\n\n5.1.1 Calculate sample mean, standard deviation and variance with equal probability\nYou can use R to calculate the sample mean, standard deviation, and variance of a given data set using built-in functions like mean(), sd(), and var(). Here’s some sample R code to do that:\n\n\nCode\n# Sample data set\ndata_set &lt;- c(12, 15, 18, 21, 24, 27, 30, 33, 36, 39)\n\n# Calculate the mean\nmean_value &lt;- mean(data_set)\ncat(\"Mean:\", mean_value, \"\\n\")\n\n\nMean: 25.5 \n\n\nCode\n# Calculate the sample standard deviation\nstd_deviation &lt;- sd(data_set)\ncat(\"Standard Deviation:\", std_deviation, \"\\n\")\n\n\nStandard Deviation: 9.082951 \n\n\nCode\n# Calculate the sample variance\nvariance &lt;- var(data_set)\ncat(\"Variance:\", variance, \"\\n\")\n\n\nVariance: 82.5 \n\n\nJust replace the data_set vector with your actual data, and this code will compute and print the mean, standard deviation, and variance for your data set. Note the results calculated by mean(), sd() and var() assumes each data points occurs with the equal probability 1/n, where n is the number of data points.\n\n\n5.1.2 Expectation and standard deviation with a given probability distribution\nCalculation by definition:\n\n\nCode\n# Define the possible values and their corresponding probabilities\nvalues &lt;- c(1, 2, 3, 4, 5)\nprobabilities &lt;- c(0.1, 0.2, 0.3, 0.2, 0.2)\n\n# Calculate the mean (expected value)\nmean_value &lt;- sum(values * probabilities)\n\n# Print the result\ncat(\"Mean (Expected Value) =\", mean_value, \"\\n\")\n\n\nMean (Expected Value) = 3.2 \n\n\nOr one can use the following built-in function:\n\n\nCode\nwt &lt;- c(5,  5,  4,  1)/15\nx &lt;- c(3.7,3.3,3.5,2.8)\nxm &lt;- weighted.mean(x, wt)\nxm\n\n\n[1] 3.453333\n\n\nTo calculate the variance of a probability distribution in R, you can use the following codes.\n\n\nCode\n# Define the values of the random variable (x_i)\nvalues &lt;- c(1, 2, 3, 4, 5)\n\n# Define the probabilities (P(x_i))\nprobabilities &lt;- c(0.2, 0.3, 0.1, 0.2, 0.2)\n\n# Calculate the mean (expected value) of the random variable\nmean_x &lt;- sum(values * probabilities)\n\n# Calculate the variance using the formula\nvariance &lt;- sum((values - mean_x)^2 * probabilities)\n\n# Print the variance\ncat(\"Variance:\", variance, \"\\n\")\n\n\nVariance: 2.09 \n\n\n\n\n5.1.3 Median\n\n\nCode\n# Create a sample vector\ndata_vector &lt;- c(12, 45, 23, 67, 8, 34, 19)\n\n# Calculate the median\nmedian_value &lt;- median(data_vector)\n\n# Print the median\ncat(\"Median:\", median_value, \"\\n\")\n\n\nMedian: 23",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "ch5.html#binomial-probability-distributions",
    "href": "ch5.html#binomial-probability-distributions",
    "title": "5  Discrete Probability Distribution",
    "section": "5.2 Binomial probability distributions",
    "text": "5.2 Binomial probability distributions\nYou can generate a data set with a binomial distribution in R using the rbinom() function. This function simulates random numbers following a binomial distribution. Here’s an example code to generate a data set with a binomial distribution:\n\n\nCode\n# Set the parameters for the binomial distribution\nn &lt;- 100    # Number of trials\np &lt;- 0.3    # Probability of success in each trial\n\n# Generate a dataset with a binomial distribution\nbinomial_data &lt;- rbinom(50, size = n, prob = p)\n\n# Print the generated dataset\nprint(binomial_data)\n\n\n [1] 26 28 28 31 29 33 27 32 35 37 30 26 31 30 34 39 22 27 26 29 33 25 31 23 28\n[26] 38 32 39 38 38 29 26 25 28 34 31 30 25 29 22 27 29 30 25 38 26 20 33 30 30\n\n\nCode\n# Create a histogram to visualize the data\nhist(binomial_data, main = \"Binomial Distribution\", xlab = \"Number of Successes\", \n     ylab = \"Frequency\", col = \"lightblue\", border = \"black\")\n\n\n\n\n\nCode\n# verify the mean =np, and var=npq\n# Sample mean\nmean(binomial_data) \n\n\n[1] 29.84\n\n\nCode\n# Theoretical mean\nn*p\n\n\n[1] 30\n\n\nCode\n# Sample variance \nvar(binomial_data)\n\n\n[1] 21.7698\n\n\nCode\n# Theoretical variance \nn*p*(1-p)\n\n\n[1] 21\n\n\nYou can calculate the probability of specific outcomes in a binomial distribution in R using the dbinom() function, which calculates the probability mass function (PMF) of the binomial distribution. Here’s how to use it:\n\n\nCode\n# Set the parameters for the binomial distribution\nx &lt;- 2     # Number of successes (the outcome you want to calculate the \n           # probability for)\nn &lt;- 10    # Number of trials\np &lt;- 0.3   # Probability of success in each trial\n\n# Calculate the probability of getting 'x' successes in 'n' trials\nprobability &lt;- dbinom(x, size = n, prob = p)\n\n# Print the calculated probability\ncat(\"Probability of\", x, \"successes in\", n, \"trials:\", probability, \"\\n\")\n\n\nProbability of 2 successes in 10 trials: 0.2334744 \n\n\nThe pbinom() function in R is used to calculate cumulative probabilities for a binomial distribution. Specifically, it calculates the cumulative probability that a random variable following a binomial distribution is less than or equal to a specified value. In other words, it gives you the cumulative distribution function (CDF) for a binomial distribution.\nHere’s the basic syntax of the pbinom() function:\n\n\nCode\npbinom(q, size, prob, lower.tail = TRUE)\n\n\nq: The value for which you want to calculate the cumulative probability.\nsize: The number of trials or events in the binomial distribution.\nprob: The probability of success in each trial.\nlower.tail: A logical parameter that determines whether you want the cumulative probability for values less than or equal to q (TRUE) or greater than q (FALSE). By default, it is set to TRUE.\nThe pbinom() function returns the cumulative probability for the specified value q based on the given parameters.\nHere’s an example of how to use pbinom():\n\n\nCode\n# Calculate the cumulative probability that X is less than or equal to 3\ncumulative_prob &lt;- pbinom(3, size = 10, prob = 0.3)\n\n# Print the cumulative probability\ncat(\"Cumulative Probability:\", cumulative_prob, \"\\n\")\n\n\nCumulative Probability: 0.6496107 \n\n\nIn this example, we’re calculating the cumulative probability that a random variable following a binomial distribution with parameters size = 10 and prob = 0.3 is less than or equal to 3. The result is stored in the cumulative_prob variable and printed to the console.\nYou can use the pbinom() function to answer questions like “What is the probability of getting at most 3 successes in 10 trials with a success probability of 0.3?” by specifying the appropriate values for q, size, and prob.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "ch5.html#poisson-probability-distributions-optional",
    "href": "ch5.html#poisson-probability-distributions-optional",
    "title": "5  Discrete Probability Distribution",
    "section": "5.3 Poisson probability distributions (Optional)",
    "text": "5.3 Poisson probability distributions (Optional)\nTo generate a data set with a Poisson distribution in R, you can use the rpois() function. The Poisson distribution is often used to model the number of events occurring in a fixed interval of time or space when the events happen with a known constant mean rate. Here’s how you can use rpois():\n\n\nCode\n# Set the parameters for the Poisson distribution\nlambda &lt;- 3  # Mean (average) rate of events\n\n# Generate a dataset with a Poisson distribution\npoisson_data &lt;- rpois(n = 100, lambda = lambda)\n\n# Print the generated dataset\nprint(poisson_data)\n\n\n  [1] 3 5 8 3 1 4 5 2 3 2 1 2 2 3 1 2 3 2 4 0 0 2 5 3 4 3 2 3 2 2 5 2 7 3 4 1 2\n [38] 4 2 1 3 3 4 4 4 3 6 2 0 4 1 2 3 5 0 0 1 2 3 5 0 2 4 4 4 2 1 4 1 3 1 1 0 2\n [75] 1 3 1 3 4 2 3 1 2 2 4 5 4 2 4 0 5 4 4 4 4 6 7 5 2 2\n\n\nCode\n# Create a histogram to visualize the data\nhist(poisson_data, main = \"Poisson Distribution\", xlab = \"Number of Events\", \n     ylab = \"Frequency\", col = \"lightblue\", border = \"black\")\n\n\n\n\n\nCode\n# Sample mean\nmean(poisson_data)\n\n\n[1] 2.81\n\n\nCode\n#Theoretical mean = lambda\n\n# Sample Variance\nvar(poisson_data)\n\n\n[1] 2.842323\n\n\nCode\n#Theoretical variance = lambda\n\n\nTo calculate the probability of a specific value occurring in a Poisson distribution in R, you can use the dpois() function. This function calculates the probability mass function (PMF) of the Poisson distribution. Here’s how to use it.\n\n\nCode\n# Set the parameters for the Poisson distribution\nx &lt;- 2     # The specific value for which you want to calculate the probability\nlambda &lt;- 3  # Mean (average) rate of events\n\n# Calculate the probability of getting exactly 'x' events\nprobability &lt;- dpois(x, lambda)\n\n# Print the calculated probability\ncat(\"Probability of\", x, \"events:\", probability, \"\\n\")\n\n\nProbability of 2 events: 0.2240418 \n\n\nTo calculate the cumulative distribution function (CDF) for a Poisson distribution in R, you can use the ppois() function. This function calculates the cumulative probability that a Poisson random variable is less than or equal to a specified value. Here’s how to use it:\n\n\nCode\n# Set the parameters for the Poisson distribution\nx &lt;- 2  # The specific value to calculate the cumulative probability\nlambda &lt;- 3  # Mean (average) rate of events\n\n# Calculate the cumulative probability of getting less than or equal to 'x' events\ncumulative_prob &lt;- ppois(x, lambda)\n\n# Print the calculated cumulative probability\ncat(\"Cumulative Probability of less than or equal to\", x, \"events:\", \n    cumulative_prob, \"\\n\")\n\n\nCumulative Probability of less than or equal to 2 events: 0.4231901",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "ch6.html#the-standard-normal-distribution",
    "href": "ch6.html#the-standard-normal-distribution",
    "title": "6  NORMAL PROBABILITY DISTRIBUTION",
    "section": "6.1 THE standard normal distribution",
    "text": "6.1 THE standard normal distribution\n\n6.1.1 Normal distribution graph (Optional)\n\n\nCode\nset.seed(123)                       # Set the seed for reproducibility\nx &lt;- rnorm(1000, mean = 0, sd = 1)  # Generate data for a standard normal distribution\n\n# Plot the data with density curve\nhist(x, probability = TRUE, col = \"lightblue\", main = \"Standard Normal Distribution\")\nlines(density(x), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n6.1.2 Find the probability (area) when z scores are given\n\n\nCode\n# Find the area under the curve to the left of a certain value: P(z&lt;1)\npnorm(1, mean = 0, sd = 1)\n\n\n[1] 0.8413447\n\n\nCode\n# Find the area under the curve to the right of a certain value: P(z&gt;1)\n1-pnorm(1, mean = 0, sd = 1)\n\n\n[1] 0.1586553\n\n\nCode\n# Find the area under the curve between two values: P(-1&lt;z&lt;1)\ndiff(pnorm(c(-1, 1), mean = 0, sd = 1))\n\n\n[1] 0.6826895\n\n\n\n\n6.1.3 Find z scores when the area is given\n\n\nCode\n# Find the value with a certain area under the curve to its left: critical value \nalpha &lt;- 0.05\nqnorm(1-alpha, mean = 0, sd = 1) # find the critical Z score.\n\n\n[1] 1.644854",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NORMAL PROBABILITY DISTRIBUTION</span>"
    ]
  },
  {
    "objectID": "ch6.html#real-application-of-normal-distribution",
    "href": "ch6.html#real-application-of-normal-distribution",
    "title": "6  NORMAL PROBABILITY DISTRIBUTION",
    "section": "6.2 REAL application of normal distribution",
    "text": "6.2 REAL application of normal distribution\n\n6.2.1 Convert an individual x value to a z-score\n\n\nCode\nx &lt;- 80  # the individual value\nmu &lt;- 75  # the mean of the distribution \nsigma &lt;- 10  # the standard deviation of the distribution \n\n# Calculate z-scores for the individual value using scale()\nz_scores &lt;- scale(x, center = mu, scale = sigma)\ncat(\"Z-score:\", z_scores, \"\\n\") # print the z-score\n\n\nZ-score: 0.5 \n\n\nCode\nz &lt;- (x - mu) / sigma  # find the z-score by using the formula \ncat(\"Z =\", z, \"\\n\") # print the z-score\n\n\nZ = 0.5 \n\n\n\n\n6.2.2 Find the probability when x value is given (page 269 Pulse Rates Question)\n\n\nCode\nx1 &lt;- 60\nx2 &lt;- 80\nmu &lt;- 69.6\nsigma &lt;- 11.3\n# Find the probability that X is less than 60: P(X&lt;60)\npnorm(x1, mean = mu, sd = sigma)\n\n\n[1] 0.1977856\n\n\nCode\n# Find the probability that X is great than 80: P(X&gt;80)\n1-pnorm(x2, mean = mu, sd = sigma)\n\n\n[1] 0.1786939\n\n\nCode\n# Find the probability between two values: P(60&lt;X&lt;80)\ndiff(pnorm(c(x1, x2), mean = mu, sd = sigma))\n\n\n[1] 0.6235205\n\n\n\n\n6.2.3 Convert a z-score back to x value\n\n\nCode\nz &lt;- 1.96  # the z-score\nmu &lt;- 100  # the mean of the distribution\nsigma &lt;- 15  # the standard deviation of the distribution\nx &lt;- z * sigma + mu  # convert the z score to individual x value using formula\ncat(\"X =\", x, \"\\n\")  # print the individual x value\n\n\nX = 129.4",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NORMAL PROBABILITY DISTRIBUTION</span>"
    ]
  },
  {
    "objectID": "ch6.html#sampling-distributions-and-estimators-optional",
    "href": "ch6.html#sampling-distributions-and-estimators-optional",
    "title": "6  NORMAL PROBABILITY DISTRIBUTION",
    "section": "6.3 SAMPLING distributions and estimators (Optional)",
    "text": "6.3 SAMPLING distributions and estimators (Optional)\n\n6.3.1 General behavior of sampling distribution of sample proportions\n\n\nCode\n# Set the seed for reproducibility\nset.seed (123)\n# Generate data\nn &lt;- 10  # sample size\np &lt;- 0.5  # population proportion\nsamples &lt;- replicate(50000, rbinom(1, size = n, prob = p))\n\n# Calculate sample proportions of successes\nsample_props &lt;- samples / n\n\n# Plot the histogram\n\nhist(sample_props, breaks = seq( 0, 1, by = 0.1 ), col = \"lightblue\", \n     main = \"Sampling Distribution of Sample Proportion\")\n\n\n\n\n\n\n\n6.3.2 general behavior of sampling distribution of sample means\n\n\nCode\n#input the parameter values\nmu &lt;- 3.5    \nsigma &lt;- 1.7       \nn &lt;- 5         \n# Simulate sampling distribution\nsample_means &lt;- replicate(10000, mean(rnorm(n, mu, sigma)))\n\n# Create a histogram of the sampling distribution of the sample mean\nhist(sample_means, breaks =\"FD\",  main = \"Sampling Distribution of Sample Mean\", \n     xlab = \"Sample Mean\", ylab = \"Frequency\", col = \"lightblue\", border = \"black\")\n\n\n\n\n\n\n\n6.3.3 general behavior of sampling distribution of sample variances\n\n\nCode\nmu &lt;- 4    # True population mean\nsigma &lt;- 8       # Population standard deviation\nsample_size &lt;- 10         # Sample size\nnum_samples &lt;- 10000       # Number of samples\n# Function to calculate sample variance\nsample_variance &lt;- function(sample) {\n  n &lt;- length(sample)\n  mean_sample &lt;- mean(sample)\n  sum_squared_deviations &lt;- sum((sample - mean_sample)^2)\n  return(sum_squared_deviations / (n - 1))\n}\n# Simulate sampling distribution\nsample_variances &lt;- replicate(num_samples, sample_variance(rnorm(sample_size, \n                                                                 mu, sigma)))\n\n# Create a histogram of the sampling distribution of sample variance\nhist(sample_variances, breaks = \"FD\", freq = FALSE, \n     main = \"Sampling Distribution of Sample Variance\",\n     xlab = \"Sample Variance\", ylab = \"Frequency\", col = \"lightblue\", border = \"black\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NORMAL PROBABILITY DISTRIBUTION</span>"
    ]
  },
  {
    "objectID": "ch6.html#the-central-limit-theorem",
    "href": "ch6.html#the-central-limit-theorem",
    "title": "6  NORMAL PROBABILITY DISTRIBUTION",
    "section": "6.4 THE central limit theorem",
    "text": "6.4 THE central limit theorem\n\n6.4.1 Find the probability when individual value is used (Page 292 Ejection Seat Question)\n\n\nCode\nmu &lt;- 171 # population mean\nsigma &lt;- 46 # population standard deviation\nn &lt;- 25 # sample size\nx_lower &lt;- 140\nx_upper &lt;- 211\n\n# Find the probability between two X values\nprobability_range &lt;- diff(pnorm(c(x_lower, x_upper), mean = mu, sd = sigma))\nprobability_range\n\n\n[1] 0.5575477\n\n\n\n\n6.4.2 Find the probability when sample mean is used (Page 292 Ejection Seat Question)\n\n\nCode\n# Find the probability between two mean values $x/bar$ (CLT)\nstandard_error &lt;- sigma / sqrt(n) # Calculate the standard error of the sample mean\nprobability_range &lt;- diff(pnorm(c(x_lower, x_upper), mean = mu, \n                                sd = standard_error))# Find the probability  \nprobability_range\n\n\n[1] 0.9996167",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>NORMAL PROBABILITY DISTRIBUTION</span>"
    ]
  },
  {
    "objectID": "ch7.html#estimating-a-population-proportion-page-313-online-course-example",
    "href": "ch7.html#estimating-a-population-proportion-page-313-online-course-example",
    "title": "7  ESTIMATING PARAMETERS AND DETERMINGING SAMPLE SIZES",
    "section": "7.1 ESTIMATING a population proportion (Page 313 Online Course Example)",
    "text": "7.1 ESTIMATING a population proportion (Page 313 Online Course Example)\n\n7.1.1 Getting the CI directly\n\n\nCode\np_hat &lt;- 0.53 # 0.53 for 53% sample proportion\nn &lt;- 950 # sample size\nsuccess &lt;- n*p_hat # number of success\n\n# Calculate a 95% confidence interval for the population proportion\nresult &lt;- prop.test(success, n, conf.level = 0.95)\n\n# Extract the confidence interval\nconf_interval &lt;- result$conf.int\n# Print the confidence interval (calculated by the Wilson method)\ncat(\"Confidence Interval:\", conf_interval[1], \"to\", conf_interval[2], \"\\n\")\n\n\nConfidence Interval: 0.4976792 to 0.5620751 \n\n\n\n\n7.1.2 Getting the CI step by step using the textbook’s Wald’s Method (slightly different result than the result given above)\n1.Critical value\n\n\nCode\n# Confidence level (e.g., 0.95 for 95% confidence)\nconfidence_level &lt;- 0.95  \n# get alpha value\nalpha &lt;- 1-confidence_level\n\n# Find the critical Z-value using qnorm()\ncritical_z &lt;- qnorm (1 - alpha/2)\n# Print the result\ncat(\"Critical Z =\", critical_z, \"\\n\")\n\n\nCritical Z = 1.959964 \n\n\n\nMargin of error\n\n\n\nCode\n# Calculate the standard error\nstandard_error &lt;- sqrt((p_hat * (1 - p_hat)) / n)\n# Calculate the margin of error\nmargin_of_error &lt;- critical_z * standard_error\n# Print the result\ncat(\"E=\", margin_of_error, \"\\n\")\n\n\nE= 0.03173753 \n\n\n\nConfidence interval\n\n\n\nCode\n# Calculate the confidence interval\nconfidence_interval &lt;- c (p_hat - margin_of_error,\n                          p_hat + margin_of_error)\n\n# Print the confidence interval\ncat(\"Confidence Interval:\", confidence_interval[1], \"to\", confidence_interval[2], \"\\n\")\n\n\nConfidence Interval: 0.4982625 to 0.5617375",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ESTIMATING PARAMETERS AND DETERMINGING SAMPLE SIZES</span>"
    ]
  },
  {
    "objectID": "ch7.html#estimating-a-population-mean",
    "href": "ch7.html#estimating-a-population-mean",
    "title": "7  ESTIMATING PARAMETERS AND DETERMINGING SAMPLE SIZES",
    "section": "7.2 ESTIMATING a population mean",
    "text": "7.2 ESTIMATING a population mean\n\n7.2.1 Get the CI directly with sample data values given. (Page 343 Mercury question)\nIn this case, the population \\sigma is unknown.\n\n\nCode\n# Calculate a 98% confidence interval for the population mean\n#Sample data \nmercury &lt;- c(0.56, 0.75, 0.10, 0.95, 1.25, 0.54, 0.88)\nresult &lt;- t.test(mercury,conf.level = 0.98)\n\n# Extract the confidence interval\nconf_interval &lt;- result$conf.int\n\n# Print the confidence interval\ncat(\"Confidence Interval:\", conf_interval[1], \"to\", conf_interval[2], \"\\n\")\n\n\nConfidence Interval: 0.2841145 to 1.153028 \n\n\n\n\n7.2.2 Get the CI step by step with given mean and standard deviation (Page 341 Hershey kisses question)\n\nCritical value\n\n\n\nCode\nconfidence_level &lt;- 0.99  # Confidence level (e.g., 0.99 for 99% confidence)\nalpha &lt;- 1- confidence_level\nn &lt;- 32         # Sample size\n\n# Calculate the degrees of freedom\ndegrees_of_freedom &lt;- n - 1\n\n# Find the critical t-value using qt()\ncritical_t &lt;- qt(1 - alpha/ 2, df = degrees_of_freedom)\n\n# Print the result\ncat(\"Critical t-value for degrees of freedom =\", degrees_of_freedom, \n    \"and confidence level =\", confidence_level, \":\", critical_t, \"\\n\")\n\n\nCritical t-value for degrees of freedom = 31 and confidence level = 0.99 : 2.744042 \n\n\n\nMargin of error\n\n\n\nCode\n# Given sample standard deviation (this is s value)\nsample_standard_deviation &lt;- 0.1077\n\n# Calculate the standard error\nstandard_error &lt;- sample_standard_deviation / sqrt(n)\n\n# Calculate the margin of error\nmargin_of_error &lt;- critical_t * standard_error\n\n# Print the result\ncat(\"Margin of Error for confidence level =\", confidence_level, \n    \"and sample size =\", n, \":\", margin_of_error, \"\\n\")\n\n\nMargin of Error for confidence level = 0.99 and sample size = 32 : 0.0522434 \n\n\n\nConfidence interval\n\n\n\nCode\nx_bar&lt;- 4.5210       # Sample mean\n\n# Calculate the lower and upper bounds of the confidence interval\nlower_bound &lt;- x_bar - margin_of_error\nupper_bound &lt;- x_bar + margin_of_error\n\n# Print the result\ncat(\"Confidence Interval:\", lower_bound, \"to\", upper_bound, \"\\n\")\n\n\nConfidence Interval: 4.468757 to 4.573243",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ESTIMATING PARAMETERS AND DETERMINGING SAMPLE SIZES</span>"
    ]
  },
  {
    "objectID": "ch7.html#estimating-a-population-deviation-or-variance-body-temperature-example-page-353",
    "href": "ch7.html#estimating-a-population-deviation-or-variance-body-temperature-example-page-353",
    "title": "7  ESTIMATING PARAMETERS AND DETERMINGING SAMPLE SIZES",
    "section": "7.3 ESTIMATING a population Deviation or Variance (body temperature example page 353)",
    "text": "7.3 ESTIMATING a population Deviation or Variance (body temperature example page 353)\n\n7.3.1 Critical values\n\n\nCode\nconfidence_level &lt;- 0.95  # Confidence level ( 0.95 for 95% confidence)\nalpha &lt;- 1- confidence_level\nsample_size &lt;- 106          # Sample size\ndegrees_of_freedom &lt;- sample_size - 1  # df for the chi-squared distribution\n\n# Find the critical values using the chi-squared distribution\nlower_critical_value &lt;- qchisq(1-alpha/2, df = degrees_of_freedom)\nupper_critical_value &lt;- qchisq(alpha/2, df = degrees_of_freedom)\n\n# Print the results\ncat(\"Lower Critical Value:\", lower_critical_value, \"\\n\")\n\n\nLower Critical Value: 135.247 \n\n\nCode\ncat(\"Upper Critical Value:\", upper_critical_value, \"\\n\")\n\n\nUpper Critical Value: 78.5364 \n\n\n\n\n7.3.2 Confidence interval\n\n\nCode\nsample_standard_deviation &lt;- 0.62                  # sample standard deviation s\nsample_variance &lt;- sample_standard_deviation^2     # Sample variance\n\n# Calculate the confidence interval for variance\nconfidence_interval &lt;- c(((sample_size - 1) * sample_variance) / lower_critical_value,\n                         ((sample_size - 1) * sample_variance) / upper_critical_value)\n\n# Print the confidence interval\nconfidence_interval\n\n\n[1] 0.2984318 0.5139273",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ESTIMATING PARAMETERS AND DETERMINGING SAMPLE SIZES</span>"
    ]
  },
  {
    "objectID": "ch8.html#basic-of-hypothesis-testing",
    "href": "ch8.html#basic-of-hypothesis-testing",
    "title": "8  Hypothesis Testing",
    "section": "8.1 Basic of Hypothesis Testing",
    "text": "8.1 Basic of Hypothesis Testing\nWe will use the following functions to perform hypothesis tests.\n\n\nCode\nlibrary(BSDA)\n# prop.test(x, n, p = NULL,\n#           alternative = c(\"two.sided\", \"less\", \"greater\"),\n#           conf.level = 0.95, correct = TRUE)\n\n# t.test(x, y = NULL,\n#       alternative = c(\"two.sided\", \"less\", \"greater\"),\n#       mu = 0, paired = FALSE, var.equal = FALSE,\n#       conf.level = 0.95, ...)\n\n# z.test(\n#   x, y = NULL,\n#   alternative = \"two.sided\",\n#   mu = 0, sigma.x = NULL, sigma.y = NULL,\n#   conf.level = 0.95)\n\n\nWe use qnorm() and qt() functions to calculate critical values. For example, we can obtain z_{0.95} using the qnorm(0.95) for a normal distribution, and the critical value t_{0.05, 5} using qt(0.95, 5) for a t-distribution with 5 degree of freedom with \\alpha=0.05 as below.\n\n\nCode\nqnorm(0.95)\n\n\n[1] 1.644854\n\n\nCode\nqt(0.95, 5)\n\n\n[1] 2.015048",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "ch8.html#testing-a-claim-about-a-proportion",
    "href": "ch8.html#testing-a-claim-about-a-proportion",
    "title": "8  Hypothesis Testing",
    "section": "8.2 Testing a Claim About a Proportion",
    "text": "8.2 Testing a Claim About a Proportion\nmtcars dataset has data for 32 automobiles in 1973-1974 with 11 variables. Among these variable, we are interested to check if the proportion of V-shaped engine (vs = 0) is 0.5. That is, H_0: p = 0.5.\n\n\nCode\ndata(mtcars)\nattach(mtcars)\ntable(vs)\n\n\nvs\n 0  1 \n18 14 \n\n\nCode\nprop.table(table(vs))\n\n\nvs\n     0      1 \n0.5625 0.4375 \n\n\n\n8.2.1 Two-sided proportion test using the z-test (method in the textbook)\nWe first check if we can use a normal approximation to perform a proportion test. With a sample size of n=32 and a proportion of interest p=0.5, both the expected number of successes and failures are np= n(1-p) = 32\\cdot 0.5 = 16. Since they are greater than 5, we can apply the proportion test using a normal approximation. In our sample, the number of success (vs=0) is 18 and the sample proportion is 0.56.\n\n\nCode\n# Example data\nsuccesses &lt;- 18  # Number of successes\ntrials &lt;- 32    # Total number of trials\nnull_prob &lt;- 0.5  # Hypothesized population proportion under the null hypothesis\n\n# Calculate the sample proportion\nsample_proportion &lt;- successes / trials\n\n# Perform the z-test\nz_stat &lt;- (sample_proportion - null_prob) / sqrt(null_prob * (1 - null_prob) / trials)\n\n# Calculate the p-value\np_value &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\n# Calculate the critical value\nalpha &lt;- 0.05\ncritical_value &lt;- c(qnorm(alpha),qnorm(1-alpha))\n\n# Print the results\ncat(\"Z-statistic:\", z_stat, \"\\n\")\n\n\nZ-statistic: 0.7071068 \n\n\nCode\ncat(\"p-value:\", p_value, \"\\n\")\n\n\np-value: 0.4795001 \n\n\nCode\ncat(\"Critical values:\", critical_value, \"\\n\")\n\n\nCritical values: -1.644854 1.644854 \n\n\nWe are ready to make a decision using the following method:\n\np-value method: The p-value 0.4795001 is greater than the significance level \\alpha =0.05, therefore we fail to reject the Null hypothesis H_0: p=0.5.\ncritical value method: The test statistics 0.7071068 is not as extreme as the two critical values, therefore we fail to reject the Null Hypothesis.\n\n\n\n8.2.2 Two-sided Proportion Test using the built-in function prop.test\nNext we will use the R built-in prop.test() function to perform one sample proportion test. The syntax is below.\n\n\nCode\n# prop.test(x, n, p = p_0, conf.level=0.95, alternative=c(\"two.sided\", \"less\", \n# \"greater\"))\n\n\nDepending on the alternative hypothesis H_1, we can choose one among two.sided, less, and greater:\n\nH_1: p \\ne p_0: alternative = \"two.sided\"\nH_1: p &lt; p_0 :alternative = \"less\"\nH_1: p &gt; p_0: alternative = \"greater\"\n\nIt is remarkable that the built-in prop.test uses the Pearson \\chi^2 distributed test statistic which is different than the z-test used by the textbook.\nH_0: p = 0.5 \\quad \\textrm{ vs }\\quad H_1: p \\ne 0.5\n\n\nCode\nres &lt;- prop.test(x=18, n=32, p = 0.50, alternative = \"two.sided\", conf.level = 0.95)\nres\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  18 out of 32, null probability 0.5\nX-squared = 0.28125, df = 1, p-value = 0.5959\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.3788033 0.7316489\nsample estimates:\n     p \n0.5625 \n\n\n\n\nCode\ncat(\"The p-value is given by \", res$p.value, \"\\n\")\n\n\nThe p-value is given by  0.5958831 \n\n\nCode\ncat(\"The $chi^2$ test statistic  is given by \", res$statistic, \"\\n\")\n\n\nThe $chi^2$ test statistic  is given by  0.28125 \n\n\nCode\ncat(\"The confidence interval is given by (\", res$conf.int[1], \",\" ,res$conf.int[2], \")\\n\")\n\n\nThe confidence interval is given by ( 0.3788033 , 0.7316489 )\n\n\nDecision:\n\nP-Value: we fail to reject the Null Hypothesis since p-value 0.596 is greater than \\alpha=0.05.\n\nCritical Value: the \\chi^2 test statistic 0.281 is not as extreme as the critical values which can be found as below. Thus, we fail to reject the Null Hypothesis.\n\n\n\n\nCode\n# the critical value can be calculated by the following code.\nc(qchisq(0.025, 1), qchisq(0.975,1))\n\n\n[1] 0.0009820691 5.0238861873\n\n\n\nConfidence Interval: the claimed proportion 0.5 falls within the confidence interval of (0.379, 0.732). Thus we fail to reject the null hypothesis.\n\n\n\n\n8.2.3 One-sided Proportion Test\nH_0: p = 0.5 \\quad \\textrm{ vs }\\quad H_1: p &gt; 0.5\n\n\nCode\nres &lt;- prop.test(x=18, n=32, p = 0.50, alternative = \"greater\", conf.level = 0.95)\nres\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  18 out of 32, null probability 0.5\nX-squared = 0.28125, df = 1, p-value = 0.2979\nalternative hypothesis: true p is greater than 0.5\n95 percent confidence interval:\n 0.4041836 1.0000000\nsample estimates:\n     p \n0.5625 \n\n\nDecision:\n\nP-Value: we fail to reject the null hypothesis since p-value 0.298 is greater than \\alpha=0.05.\n\nCritical Value: the \\chi^2 test statistic 0.281 does not fall in the critical region which is greater than 3.8414588 or smaller than 0.0039321. Thus, we fail to reject the null hypothesis. The critical value can be found by\n\n\n\nCode\n# the critical value can be calculated by the following code.\nc(qchisq(0.05,1), qchisq(0.95,1))\n\n\n[1] 0.00393214 3.84145882\n\n\n\nConfidence Interval: the claimed proportion 0.5 falls within the confidence interval of (0.404, 1). Thus we fail to reject the null hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "ch8.html#testing-a-claim-about-a-mean",
    "href": "ch8.html#testing-a-claim-about-a-mean",
    "title": "8  Hypothesis Testing",
    "section": "8.3 Testing a Claim About a Mean",
    "text": "8.3 Testing a Claim About a Mean\n\n8.3.1 Unknown \\sigma with Normality Assumption\nWe use one sample t-test with t.test() function when we assume normality for population or the sample size is large enough. The syntax is as below if we want to test with a sample vector (variable) x for H_0: \\mu = m with a given confidence level conf.level, for example, conf.level=0.95.\n\n\nCode\n# t.test(x, mu= m, conf.level=0.95, alternative=c(\"two.sided\", \"less\", \"greater\"))\n\n\nDepending on the alternative hypothesis H_1, we can choose one among two.sided, less, and greater.\n\nH_1: \\mu \\ne m: alternative = \"two.sided\"\nH_1: \\mu &lt; m :alternative = \"less\"\nH_1: \\mu &gt; m: alternative = \"greater\"\n\nAs an example, we test for mpg with H_0: \\mu = 22. That is, we test if the population mean of mpg is equal to 22. mtcars cars have 32 samples and the sample size is large enough to use t-test with \\alpha = 0.05.\n\n8.3.1.1 Two-sided t-test\nH_0: \\mu = 22 \\quad \\textrm{ vs }\\quad H_1: \\mu \\ne 22\n\n\nCode\nres &lt;- t.test(mpg, mu=22, alternative = \"two.sided\", conf.level = 0.95)\nres\n\n\n\n    One Sample t-test\n\ndata:  mpg\nt = -1.7921, df = 31, p-value = 0.08288\nalternative hypothesis: true mean is not equal to 22\n95 percent confidence interval:\n 17.91768 22.26357\nsample estimates:\nmean of x \n 20.09062 \n\n\n\n\nCode\ncat(\"The p-value is given by \", res$p.value, \"\\n\")\n\n\nThe p-value is given by  0.08287848 \n\n\nCode\ncat(\"The test statistic is given by \", res$statistic, \"\\n\")\n\n\nThe test statistic is given by  -1.792127 \n\n\nCode\ncat(\"The confidence interval is given by (\", res$conf.int[1], \",\" ,res$conf.int[2], \")\\n\")\n\n\nThe confidence interval is given by ( 17.91768 , 22.26357 )\n\n\nDecision:\n\nP-Value: we fail to reject the null hypothesis since p-value 0.083 is greater than \\alpha=0.05.\nCritical Value: the test statistic t= -1.792 is closer to 0 than the critical values which can be found as below. Thus, we fail to reject the null hypothesis.\n\n\n\nCode\n# the critical value can be calculated by the following code.\nc(qt(0.025, df=31), qt(0.975, df=31))\n\n\n[1] -2.039513  2.039513\n\n\n\nConfidence Interval: the claimed mean 22 falls within the confidence interval of (17.918, 22.264). Thus we fail to reject the null hypothesis.\n\n\n\n8.3.1.2 One-sided t-test H_1: \\mu &lt; m\nH_0: \\mu = 22 \\quad \\textrm{ vs }\\quad H_1: \\mu &lt; 22\n\n\nCode\nres &lt;- t.test(mpg, mu=22, alternative = \"less\", conf.level = 0.95)\nres\n\n\n\n    One Sample t-test\n\ndata:  mpg\nt = -1.7921, df = 31, p-value = 0.04144\nalternative hypothesis: true mean is less than 22\n95 percent confidence interval:\n     -Inf 21.89707\nsample estimates:\nmean of x \n 20.09062 \n\n\nDecision:\n\nP-Value: we reject the null hypothesis since p-value 0.041 is less than \\alpha=0.05.\n\nCritical Value: the test statistic t= -1.792 falls in the critical region which is less than t_{0.05, 31} = -1.696. Thus, we reject the null hypothesis.\n\n\n\n\nCode\n# the critical value can be calculated by the following code.\nqt(0.05, df=31)\n\n\n[1] -1.695519\n\n\n\nConfidence Interval: the claimed mean \\mu=22 does not fall within the confidence interval of (-\\infty, 21.897). Thus we reject the null hypothesis.\n\n\n\n\n8.3.2 Known \\sigma with Normality Assumption\nWe use one sample z-test or normal test with z.test() function when we assume normality for population with known population standard deviation \\sigma. The syntax is as below if we want to test with a sample vector (variable) x for H_0: \\mu = m with \\alpha = 0.05 and a known sigma.\n\n\nCode\n#library(BSDA)\n# z.test(x, mu = m, sigma.x = sigma, conf.level = 0.95, \n# alternative = c(\"two.sided\", \"less\", \"greater\"))\n\n\nDepending on the alternative hypothesis H_1, we can choose one among two.sided, less, and greater.\n\nH_1: \\mu \\ne m: alternative = \"two.sided\"\nH_1: \\mu &lt; m :alternative = \"less\"\nH_1: \\mu &gt; m: alternative = \"greater\"\n\nFor example, we test for mpg with H_0: \\mu = 22. Assume mpg follows a normal distribution with \\sigma = 6, then we can use z-test with \\alpha = 0.05.\n\n8.3.2.1 Two-sided z-test\nH_0: \\mu = 22 \\quad \\textrm{ vs }\\quad H_1: \\mu \\ne 22\n\n\nCode\nlibrary(BSDA)\nres &lt;- z.test(mpg, mu=22, sigma.x = 6, alternative = \"two.sided\", conf.level = 0.95)\nres\n\n\n\n    One-sample z-Test\n\ndata:  mpg\nz = -1.8002, p-value = 0.07183\nalternative hypothesis: true mean is not equal to 22\n95 percent confidence interval:\n 18.01177 22.16948\nsample estimates:\nmean of x \n 20.09062 \n\n\n\n\nCode\ncat(\"The p-value is given by \", res$p.value, \"\\n\")\n\n\nThe p-value is given by  0.07183285 \n\n\nCode\ncat(\"The test statistic is given by \", res$statistic, \"\\n\")\n\n\nThe test statistic is given by  -1.800176 \n\n\nCode\ncat(\"The confidence interval is given by (\", res$conf.int[1], \",\" ,res$conf.int[2], \")\\n\")\n\n\nThe confidence interval is given by ( 18.01177 , 22.16948 )\n\n\nDecision:\n\nP-Value: we fail to reject the null hypothesis since p-value 0.072 is greater than \\alpha=0.05.\nCritical Value: the test statistic z= -1.8 is closer to 0 than the critical values as found below. Thus, we fail to reject the null hypothesis.\n\n\n\nCode\n# the critical value can be calculated by the following code.\nc(qnorm(0.025), qnorm(0.975))\n\n\n[1] -1.959964  1.959964\n\n\n\nConfidence Interval: the claimed mean 22 falls within the confidence interval of (18.012, 22.169). Thus we fail to reject the null hypothesis.\n\n\n\n8.3.2.2 One-sided z-test H_1: \\mu &lt; m\nH_0: \\mu_{mpg} = 22 \\quad \\textrm{ vs }\\quad H_1: \\mu_{mpg} &lt; 22\n\n\nCode\nres &lt;- z.test(mpg, mu=22, sigma.x = 6, alternative = \"less\", conf.level = 0.95)\nres\n\n\n\n    One-sample z-Test\n\ndata:  mpg\nz = -1.8002, p-value = 0.03592\nalternative hypothesis: true mean is less than 22\n95 percent confidence interval:\n       NA 21.83526\nsample estimates:\nmean of x \n 20.09062 \n\n\nDecision:\n\nP-Value: we reject the null hypothesis since p-value 0.036 is less than \\alpha=0.05.\nCritical Value: the test statistic z= -1.8 falls in the critical region which is less than z_{0.05} = -1.645. Thus, we reject the null hypothesis.\n\n\n\nCode\n# the critical value can be calculated by the following code.\nqnorm(0.05)\n\n\n[1] -1.644854\n\n\n\nConfidence Interval: the claimed mean does not fall within the confidence interval of (-\\infty, 21.835). Thus we reject the null hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "ch9.html",
    "href": "ch9.html",
    "title": "9  INFERENCE FROM TWO SAMPLES",
    "section": "",
    "text": "There is no content for this chapter.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>INFERENCE FROM TWO SAMPLES</span>"
    ]
  },
  {
    "objectID": "ch10.html#correlation",
    "href": "ch10.html#correlation",
    "title": "10  Correlation and Regression",
    "section": "10.1 Correlation",
    "text": "10.1 Correlation\nWe check if a linear correlation exists between two variables using cor() function.\n\n\nCode\n# We can calculate the correlation coefficient between x and y with the following code.\n# cor(x, y)\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\ndata(\"mtcars\")\nnames(mtcars)\n\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\n\n\nCode\nattach(mtcars)\n# positive correlation\nqplot(wt, disp, data = mtcars) +\n  geom_text(aes(x=2, y=400, label=\"r = 0.888\"))\n\n\n\n\n\nCode\ncor(wt, disp)\n\n\n[1] 0.8879799\n\n\nCode\n# negative correlation\nqplot(mpg, wt, data = mtcars)  +\n  geom_text(aes(x=30, y=5, label=\"r = - 0.868\"))\n\n\n\n\n\nCode\ncor(mpg, wt)\n\n\n[1] -0.8676594\n\n\nCode\n# no correlation\nqplot(drat, qsec, data = mtcars)  +\n  geom_text(aes(x=4.5, y=22, label=\"r = 0.091\"))\n\n\n\n\n\nCode\ncor(drat, qsec)\n\n\n[1] 0.09120476\n\n\n\nwt and disp have a positive correlation with r =0.888.\nwt and disp have a negative correlation with r = -0.868.\nwt and disp does not have a significant correlation with r = -0.175.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "ch10.html#linear-regression",
    "href": "ch10.html#linear-regression",
    "title": "10  Correlation and Regression",
    "section": "10.2 Linear Regression",
    "text": "10.2 Linear Regression\nAssume we have a data set data with x and y variables and we model their relationship by linear regression. We can find the slope and the intercept of the estimated regression line using the following code.\n\n\nCode\n# res &lt;- lm(y ~ x, data)\n# summary(res)\n\n\nFor example, we can find the regression line equation between disp (x-variable, predictor) and wt (y-variable, response) as below.\n\n\nCode\ndata(\"mtcars\")\nres &lt;- lm(wt ~ disp, mtcars)\nsummary(res)\n\n\n\nCall:\nlm(formula = wt ~ disp, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.89044 -0.29775 -0.00684  0.33428  0.66525 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.5998146  0.1729964   9.248 2.74e-10 ***\ndisp        0.0070103  0.0006629  10.576 1.22e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4574 on 30 degrees of freedom\nMultiple R-squared:  0.7885,    Adjusted R-squared:  0.7815 \nF-statistic: 111.8 on 1 and 30 DF,  p-value: 1.222e-11\n\n\nThe estimated regression line is wt = 1.600 + 0.007 disp since the intercept is 1.6 and the slope is 0.007. Both of them are significantly different from 0 with a significance level \\alpha = 0.05 because their p-values are almost 0. The linear relation means that one inch increase in disp (displacement) makes 7 lbs increase in wt (weight). On average, if a car has a one-inch longer displacement, it is 7 pounds heavier.\nIf a car has 200 inches displacement, then its estimated weight can be calculated as  1.600 + 0.007\\cdot200 = 3000 \\textrm{ lbs} \nWe next use the R package ggplot to visualize the data set and the regression line.\n\n\nCode\nggplot(mtcars, aes(x=disp, y=wt)) +  # define x and y\n  geom_point()+                      # scatter  plot\n  geom_smooth(method=lm, se=FALSE) + # add a regression line\n  geom_text(aes(x = 150, y = 4.5, label = \"wt = 1.600 + 0.007disp\")) #add a label",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Triola, Mario F. 2022. Elementary Statistics. USA: Pearson.",
    "crumbs": [
      "References"
    ]
  }
]